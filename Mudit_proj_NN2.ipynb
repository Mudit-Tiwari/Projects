{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, h5py\n",
    "import matplotlib.style as style; style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Metrics and preprocessing\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TF and Keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 32, 32) (42000,)\n",
      "Validation set (60000, 32, 32) (60000,)\n",
      "Test set (18000, 32, 32) (18000,)\n",
      "\n",
      "\n",
      "Unique labels in y_train: [0 1 2 3 4 5 6 7 8 9]\n",
      "Unique labels in y_val: [0 1 2 3 4 5 6 7 8 9]\n",
      "Unique labels in y_test: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "h5_SVH =h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "X_train = h5_SVH['X_train'][:]\n",
    "y_train_o = h5_SVH['y_train'][:]\n",
    "X_val = h5_SVH['X_val'][:]\n",
    "y_val_o = h5_SVH['y_val'][:]\n",
    "X_test = h5_SVH['X_test'][:]\n",
    "y_test_o = h5_SVH['y_test'][:]\n",
    "h5_SVH.close()\n",
    "\n",
    "print('Training set', X_train.shape, y_train_o.shape)\n",
    "print('Validation set', X_val.shape, y_val_o.shape)\n",
    "print('Test set', X_test.shape, y_test_o.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('Unique labels in y_train:', np.unique(y_train_o))\n",
    "print('Unique labels in y_val:', np.unique(y_val_o))\n",
    "print('Unique labels in y_test:', np.unique(y_test_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x324 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
     ]
    }
   ],
   "source": [
    "# Visualizing first 10 images in the dataset and their labels\n",
    "plt.figure(figsize = (15, 4.5))\n",
    "for i in range(10):  \n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((32, 32)),cmap = plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace = -0.1, hspace = -0.1)\n",
    "plt.show()\n",
    "\n",
    "print('Label for each of the above image: %s' % (y_train_o[0 : 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first image and label in training set\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2df5BU1ZXHv0cDgvwYBGbIMKCITkQYEHADJKAorgZNEbViTLS0YkzMJixVsdZNxdqtyrrZ3ao12fyoSm2yCWqtiYnGCBj8GYhGQQ3+AIfhx4ADAwozwDACwyAhiNz9o/v2vn5zz5meZrrHvfv9VFG8d+6c17df95n75px7zhHnHAghcXJaX0+AEFI6aOCERAwNnJCIoYETEjE0cEIi5iOlunBHRwfd84SUkYqKCknLTmkFF5H5IrJVRLaJyN2nci1CSO9TtIGLyOkA/hPA1QAmArhJRCb21sQIIafOqTyizwCwzTnXDAAi8giAawFsTv/gnj17AABHjhzB4MGDAQD9+/dXL3zGGWcE5aedpv8+ev/999Wx9957r8v5oEGDAADWRh9tjgMGDFB1rLF+/frlnTc1NaG2thYAcPLkSVXvxIkT6lhHR0dQfvjwYVXHeq1hw4apYxUVFerYsWPHgvIzzzxT1Unfq8bGRlx44YUAgL/85S+qnnU/tM/T+n7s3btXHdu5c6c6tmvXLnXs4MGD6lj6e+AZO3Zs3nldXR02btwIABg/fnxQ56KLLlJfBwCk2J1sInIDgPnOua9kz28FMNM5twjI/xu8qampqNcghNj4BQII/w1+Kit4l4sBCP628Ks2V3Cu4Em4gudTzAreHafiZNsNIDmjMQBaT+F6hJBe5lRW8NcB1IrIuQBaAHwBwM2hHzzrrLMAZFZwf2ytnEePHg3Krd+01m/T9Go2bdo0vPTSSwDsJ4mqqqqg/Nxzz1V1tKcPADh+/Lgqa29vV/Ws9639+bNp0yZVp7OzM+984cKF+MlPfgLAXqXTK0ySiRPD/tULLrhA1Tn99NO7yKyV1mN9dz744IOgvKWlRdVZs2ZN3vns2bPx8ssvAwD+9Kc/qXpbtmxRx7TvMACIhB5+gcrKyrzzH/3oR7jvvvsAAJMmTQrqdPc3eNEG7pw7ISKLAPwewOkAHnDO6d8qQkjZOaWNLs65pwE83UtzIYT0MtyqSkjE0MAJiRgaOCERQwMnJGJKlk2WJLmxwh9bG0J27NgRlL/44ouqzptvvqmO7d+/P+982rRp+PWvfw0AGDhwoKo3Y8aMoNwKJWmhNSAcCvMbItauXavqvfLKK+qY9r6tsFs6XLdw4UI899xzAOxNMH6TUohLL700KJ83b56qM3Xq1C6yffv2AbDv40c+on9tDx06FJTX19erOsuXL887nz17dk62detWVU8LdwH2hiFtE09jY6Mq08J83/3ud9XXAbiCExI1NHBCIoYGTkjE0MAJiRgaOCERUxYvejLZwx9b3lotucLa3K953oFwSqX3So4cOVLV07y1VoKKlcqYnuOIESNyMp/cECKdDJFES9M877zzVJ1QCufFF18MAGhublb1vIc7xDPPPBOUp1N1k6QjKTU1NbnP2PJCW6xfvz4oX7FihaoTikR4WSghxvOJT3xCHbvsssvUMS3CsXLlyi4ynya6bds29XoWXMEJiRgaOCERQwMnJGJo4IREDA2ckIihgRMSMWUJkyXrbPljXys9hBaqeffdd1UdKxwTCgt52YQJE1S96dOnB+XnnHOOqnPkyBF1LJ1MMGfOnJxsw4YNqp4WCgMylTdD3HTTTapOKDR45513AgCWLl2q6j3xxBPqWGtruN7munXrVJ30va+pqcndB+tzsWqyNTQ0BOWbN3cp158jVMfNy6xwnVVvbs6cOeqYdq9C33sf7rTClxZcwQmJGBo4IRFDAyckYmjghEQMDZyQiKGBExIxZQmTJWtQ+WMrbKHVJ7Paz1ihpFA7IV9PywqDjBs3Lii3apNZnVTTobA5c+bkZG+99Zaqp7XjscaGDh2q6qRDa83NzTmZNY/Vq1erY9pns3v3blUn3V5p/vz5OdnMmTNVPa15H6A3C7TaP4XaJXmZVTvQt+EKcfbZZ6tjWhPN4cOHqzKrJZbFKRm4iOwE0AngAwAnnHN/dSrXI4T0Lr2xgl/unNNLeBJC+gz+DU5IxIi17a9bZZEdAA4CcAB+5pz7uR/r6OjIXdj6u5QQUjy1tbW544qKii6F2k/1EX22c65VRKoArBSRLc65Vekf8g4t51zu2Opf/eyzzwblocLwnlBZJk+6UcHixYtxxx13AAAuv/xyVe8rX/lKUG71B7caMNx///1551//+tfx05/+FIC9z9tyss2dOzcoX7hwoaqT3mPf3NycKw1k7UX3cw2hfZ5WeatPf/rTeed33XUXvv/97wMAbr/9dlXPcrL5ftppnnrqKVUn3Yhg1apVuUYOmqMVAG655RZ1zJq/5nh84IEHulzDyx5//PGgjmUTwCk+ojvnWrP/twFYBiDcCoQQ0icUvYKLyCAApznnOrPHVwH4Tuhn/W/I/v37546tx3atwJzWlgaw29mECiF6WbqNTyHXtEJyVlgolEHnZaFQjcf6M6qtrS0ot54kQtlk77zzDgA7BGi1E9KynazPLFQo08u2b9+u6oWyAz1aQUPrMwvh77n19GR9ZhZa6C3U0ihpO8VwKo/oowAsyz5yfwTAr51z4WdrQkifULSBO+eaAVzUi3MhhPQyDJMREjE0cEIihgZOSMTQwAmJmLJkk/mCiP37988dW0UStX5QAwcOVHWskIUVfrDCIFoIzZq7teHm4MGDqkzLMALsMJlWiHLXrl2qTrow5IABA3IyK0xm3X9r84nGgQMHVJkW7gLsDEDtM7M+59D78mEp63OxCmyGvnMerS9f6HP2MmseFlzBCYkYGjghEUMDJyRiaOCERAwNnJCIKYsXPekV98c1NTXqzydzXJOEkhM8VpJHCO91t7y/2gb/IUOGqDqWp9ni6NGj6pjlAU6nwnpCCTaedLLGyZMnczLL+2vR02QOIHx/vcz6XKw6aVrtMssL7VOYQzLrtSys+6+9t9BreZl1PQuu4IREDA2ckIihgRMSMTRwQiKGBk5IxNDACYmYsoTJKisrAWRCMP74oov0YjBaSMBKQLDCO6FwTCHlorXwlJWQMWHCBHVsxoyuNSm9zEpSOXz4sDrm72easWPHqjrpMF9HR0dOpiWvAHZCTzH1yUIhLS+zWi+NHj1aHRs1alRQbtXsCyUPeZkVnrLaCRXTbsqi2PpvXMEJiRgaOCERQwMnJGJo4IREDA2ckIihgRMSMWUJk/lWOS0tLbljKyPLt9HpLUKZRF5mhde00JUV5pg4caI6ds011xQkS2OFyc4555ygfPbs2arOiBEj8s47OjpyMqullJXhpYW1rOy6UEahl4XaK3nOO+88dSzdWNGzdu1aVWfr1q1dZD6bzAobhlpReVpbW9WxP//5z0G51WKrmJp3QAEruIg8ICJtIrIxIRsuIitFpCn7/1lFvTohpKQU8oj+3wDmp2R3A3jOOVcL4LnsOSHkQ0a3Bp7t952ub3stgAezxw8CuK6X50UI6QWkkC2bIjIOwJPOubrs+SHn3LDE+EHnXN5jekdHR+7C1t91hJDiSVY/qqio6FKapixONu84aWlpyR1bzq3nn38+KH/sscdUnfr6enUsXQrn0UcfxY033ggAuPLKK1W9L33pS0G55sgB7L3Lq1evzjsfMWJEzonz9NNPq3rFONnmzZun6tTV1eWdNzc3Y/z48QCANWvWqHr33XefOvbyyy8H5dYe6jlz5uSdf/vb38Z3vpNpMX/rrbeqelOmTFHHfv/73wflixcvVnXSTrZXXnkFn/zkJwHYzr6rr75aHbPmrznZli1blnf+1a9+FT//+c8BAM8+G+7MvWHDBvV1gOLDZPtEpBoAsv+Hu9ATQvqUYlfw5QC+CODfs///znyRRCaPP9batwB66yJLxwojWEX1etryqDsdbe5A15WnpaUlJ9NWYqC4911dXd2jOfpwltWWqZjCf1bm3ZgxY1SZNX9rVb3ggguC8mnTpqk6oSxFHza07odVBPTFF19Ux7SWR5s3b1ZlVpski0LCZA8D+BOAC0Rkt4h8GRnDvlJEmgBcmT0nhHzI6HYFd87dpAxd0ctzIYT0MtyqSkjE0MAJiRgaOCERQwMnJGLKstElGV7xx1ZGlhYWssJToVCYJxQWssJZ3WHNw9oZaIXrrF5tVnhKez3r/YXCQl62d+9eVc8qDKnNUSuCCIRDg17W0zCfRyvIaG38CW0kmjlzJgDg1VdfVfXWr1+vjjU3N6tjWmizra3rdhK/kcUqGmnBFZyQiKGBExIxNHBCIoYGTkjE0MAJiRgaOCERU5YwWSibzApraVghqGL6PXWHFg6zsruseYTCI15m5cdbY1rI5dixY6pOqJCgl1kFLw8cSBf2+V+04opWD7pQXreXnXnmmare8ePH1bHhw4cH5VavtrPPPluVhQoyerZs2aKOWX3LzjorXMIwFIb01ynGXgCu4IREDQ2ckIihgRMSMTRwQiKGBk5IxJTFi55sHeSPLW9zqNUQYHsSreuFvO9eZnmbNW+5tfHfSkQJ6RWSRGB5jYuZx9tvv513XlNTk5O1tLSoelbSy6RJk4LyT33qU6rOhRdemHfe3t6ek1leaOu9ad8drZIpEE428TIrsSVdnTbJ5MmT1bHKysqg/LXXXusi822atm3bpl7Pgis4IRFDAyckYmjghEQMDZyQiKGBExIxNHBCIqbParJZiSP9+/cPyq32RIW+flpmJXJoYTkrtGVdLxTC8TJLz7pX2j1Jh8KSrFu3Lu+8pqYmJ7NqsmnhHQCYO3duUD5r1qyCr9fe3p6TdXZ2qnpWuE5LlrFaCaUbLi5atCgns8KoM2bMUMeuu07vqK21qaqoqOgi880xrc/FopDWRQ+ISJuIbEzI7hGRFhGpz/67pqhXJ4SUlEIe0f8bwPyA/IfOuanZf3rvW0JIn9GtgTvnVgHQE4EJIR9axPr7LvdDIuMAPOmcq8ue3wPgNgCHAbwB4C7n3MGkTkdHR+7CTU1NvTVfQkiC2tra3HFFRUUXp1GxBj4KQDsAB+BfAFQ7525P6iQN3Dsqdu3alausYVVFeeGFF4LyBx98UNVpaGhQx9L7mn/729/ic5/7HABg+vTpqt7Xvva1oHz27NmqjtVPesCAAXnnW7duzfWzLraii1b5xHKyLVu2LO98wYIFeOKJJwAAq1evVvUs59b8+aG/4oAbb7xR1Uk3N2hsbMztRe9tJ1v6PSfx793z0EMP4ZZbbgFQvJPthhtuUMc0J9tTTz2Vd/6Zz3wGy5cvBwD88pe/DOrU19fnjkMGXlSYzDm3zzn3gXPuJIDFAPR3SgjpM4oKk4lItXNuT/b0egAbrZ9PZv9YmUCeYrLJrCcRK0xmZaFZK6eGFUKz5mE90ViZVVqW1ObNm1Ud3w7Hs2DBgpzMuh8XX3yxOnbJJZcE5SNHjlR1Qu/Zy6wsrqNHj6pj2vsOZWp5duzYocq0+mmA/SQxZMgQdSz5WJ3EauVkzcOiWwMXkYcBXAZgpIjsBvBPAC4TkanIPKLvBPA3Rb06IaSkdGvgzrmbAuL7SzAXQkgvw62qhEQMDZyQiKGBExIxNHBCIqYs2WTJEJY/tkJe2pilU2x4ygoLaSE9KyRnzTF0PT8PKyxksXPnzqD8jTfeUHXa2tpUWaiNj+fSSy9Vx7TWQFYoKXQfC7kfR44cUce04oTbt29XdUJFLb1MC9kCvfs96G4egwYNUq9nwRWckIihgRMSMTRwQiKGBk5IxNDACYkYGjghEVOWMFkyhFVILy4rxKBhhbusrCVLTwvx9PS1CpmHdV/27dunjr3++utBuRUWGj58uCqzQmFTpkxRxwYPHhyUW33VQiEon8FnZdB1dHSoY62trUG5lYEWCkF5mTUPrTgoAAwcOFAd08Jroe+Vl1nhOguu4IREDA2ckIihgRMSMTRwQiKGBk5IxJTFi570ihfiIdc8hlYCQk+TAvzPWwkDmkfcSqCwvOih+ReSZGJ50bVqsocPH1Z1Zs6c2UU2ceJEAMDll1+u6lVVValjmrfc+lysVk4WVl0/7X1bn3Oxn4v1Xba+I9q101V3kzLLm2/BFZyQiKGBExIxNHBCIoYGTkjE0MAJiRgaOCERU5YwWTL04Y+1ljuAHmLo16+fqmOFEUKb+P3PW+EQbcxKMrDCI6EwiE/SaG9vV/WSDebSaM32KisrVZ1Zs2apMq22GmC3ctIScKyQ1rvvvttF5pNFhg4dqupZ4SmtGaNF6H15mfV5Wt9ha0xLfAmF8rysZGEyERkrIn8UkUYR2SQi38jKh4vIShFpyv5fXPMkQkjJKOQR/QQy/b8vBDALwN+KyEQAdwN4zjlXC+C57Dkh5ENEtwbunNvjnFuXPe4E0AigBsC1AHzD7gcBXFeqSRJCikOsLXxdflhkHIBVAOoAvOOcG5YYO+icyz2md3R05C7c1NTUG3MlhKRItiKuqKjo4pwo2MkmIoMBLAFwp3PucE+qrvhi+u+8807u2HJCrFmzJij/zW9+o+qsX79eHUs7gB577DHccMMNAIC6ujpV77bbbgvK58+fr+r0xMm2cePG3OtbTrYlS5aoY08++WRQbvWTvvnmm/POx4wZg927dwMArrrqKlXPcrJpzjRLJ+1kO3HiRK6yjeVke/vtt9WxxYsXB+XPPPOMqpP+Lr700kuYM2cOANtZOX36dHXsjjvuUMcmT54clK9YsSLvfNq0aXjzzTcBAMuXLw/qPPTQQ+rrAAWGyUSkHzLG/Svn3NKseJ+IVGfHqwF0bZdBCOlTul3BJbNU3w+g0Tn3g8TQcgBfBPDv2f9/p10jGW4qJEunmPpTVhaXFQaxniTee++9oNx6erFqkIVCSf413nrrLVXPCpNpc7GeTM4///y882PHjnWRhbCeTrRwnV+BQjQ2Nuad33zzzfjFL34BIP/RM01NTY06NmzYsKA8FKL0hJ4+/JNEsSE563uuhRSt7Lpi6hQChT2izwZwK4ANIuK/af+AjGE/KiJfBvAOgM8VNQNCSMno1sCdcy8B0H59XNG70yGE9CbcqkpIxNDACYkYGjghEUMDJyRiypJNltwt54+tEEMxoY5iWxdZITktNGHt/rMyzUIFAX2YbPPmzarenj171DHtXo0fP75gnb179+ZkWmiwu7GDBw8G5db7Cu1w9OFC6/vhC0SGqK6uDsorKipUnWPHjnWR+cxFK9xlbeKxWiVpIV3re9qTHadJuIITEjE0cEIihgZOSMTQwAmJGBo4IRFDAyckYsoSJku6//2xlR3jM3kKlQN2GMEqZmcVctRCaD3tc+UJ5Xx72fbt21W9/fv3q2PaHLXsLgD4wx/+kHdeV1eXk1k9zaywkDZm5emHwn/+Pmg50wAwatQodczXG0hj5ce3tLR0kfnMOet+WGFDq9ikFgIcOHCgKrOuZ8EVnJCIoYETEjE0cEIihgZOSMTQwAmJmD5LNrE8slqdNKsmWLFY89C8pJaO5enfu3dv3nlVVVVOlh5LYnlyOzs7g/J9+/apOumEmB//+Mf42c9+BsD2DFsRAi2BwvLmh+6VjypYkYqQt9kzYcKEoHzatGmqzqFDh7rIxowZAwA4cOCAqmfV87Oq5Gpjoc/Sy6zPxYIrOCERQwMnJGJo4IREDA2ckIihgRMSMTRwQiKmLGGyM844o8uxFU7SElEsnZ4movhQkdXySNvgb238t2qypZNGqqqqcrJQqMZj1ZsL1RMDujb2SxK6H1u3blXHPNZ7K6bdVCjpwsu09wXY93/cuHFB+dy5c1Wd0L2aMmUKAOD1119X9dra9HZ8q1atUse0xCLfANLz8Y9/HM8//zwAO+xp0e2nIiJjReSPItIoIptE5BtZ+T0i0iIi9dl/1xQ1A0JIyShkBT8B4C7n3DoRGQJgrYiszI790Dn3H6WbHiHkVCikN9keAHuyx50i0ghAb+9ICPnQID2ptywi4wCsAlAH4O8A3AbgMIA3kFnlc8WxOzo6chcO1b8mhJw6yTbLFRUVXZxXBRu4iAwG8CKAf3POLRWRUQDaATgA/wKg2jl3u//5pIH719i2bVuuD7XlOHrttdeC8ocffljVWb16tTqWdso8/vjjuO666wDYfag/+9nP9kgOAIMGDVLHli1blnc+adIkbNq0CQDwyCOPqHreARZCc0ZZ++XTn/mKFStw1VVXBceSFONks5xlQ4cOzTtfsmRJ7t5ef/31qt6iRYvUsSFDhgTlL7zwgqqzdOnSvPNvfvOb+N73vgfAdrJZ98M76UJo/c3TTrZvfetbuPfeewHolXGS8wsZeEGuTxHpB2AJgF8555YCgHNun3PuA+fcSQCLAcwo5FqEkPLR7d/gkolZ3Q+g0Tn3g4S8Ovv3OQBcD2Cjdo1kFpg/1rKgAD3EY4WSLEKri5dpLXcAve1OXV2dqqP9dga61iCbNGlSTmZlk1lZS9qTkLUSh0KKXmaFG636dcePHw/KtRUVsGuQWU8gVghQu//Tp09XdTo6OrrIrrjiCvXnPQ0NDUWNaU9kofe8YcMGAHZNOYtCvOizAdwKYIOI1Gdl/wDgJhGZiswj+k4Af1PUDAghJaMQL/pLAEI7T57u/ekQQnoTblUlJGJo4IREDA2ckIihgRMSMWXJJkuGt/zxli1b1J/3mz/S7Nq1S9WxNs6EstO8LBQi8ezcuTMo37hRjQia4alQ4Twvs7KxBgwYoI5pmy2skFYog27kyJHdvpb13rSwVjKTME1ojn7zi1VYsSebeDwf/ehHVZ158+blnR86dCgn07LTAGDlypXqWHNzc4/nGLr3PrynFZPsDq7ghEQMDZyQiKGBExIxNHBCIoYGTkjE0MAJiZg+601mFc7TqKqqUsfGjh2rjoXCDzNnzgRgZ2r5/lRp0nnMSYYNG6aOTZ48WZVZvaesjDctW8vqIxYKrS1YsAAAUFFRoepZaGE+K1wX4vOf/zyA8L3yfOxjH+vRNQH7+5bO1Dp06FBOdskll6h6Vs631ZNNy7wLfS533nknAGDw4MHq9Sy4ghMSMTRwQiKGBk5IxNDACYkYGjghEUMDJyRielQXvSckyyb7jK22trZcqMsK/bS3twflVujHCiOk36OImJlRHi2jycp0srKn0hlv+/fvR2VlZXCsULR7ovV3A7qGrlpaWnLFCq2wljWmzcPK/EqPtba2YvTo0QDs7Lqeht4A+/6mC002Nzdj/PjxAML90zzJYqJprPet9cNLy3ft2pUL/2o6yfkVXTaZEPJ/Exo4IRFDAyckYmjghEQMDZyQiClLsklbW1uXY+8tDaEleVie7554fzdv3oxJkyYBsL2r2tjRo0dVHav5YNr7vn//flRXVwfnmMTyiGtztDy8ocQW7/23mgVa19QiC5Y3PBRx8DLrPVtjmrfZ+u6Ekj+8zPp+WO/NuldasknotXwyVCFe9BDdruAiMkBEXhOR9SKySUT+OSs/V0ReFZEmEfmNiOitFgkhfUIhj+h/ATDPOXcRgKkA5ovILAD3Avihc64WwEEAXy7dNAkhxdCtgbsMR7Kn/bL/HIB5AB7Lyh8EcF1JZkgIKZqCdrKJyOkA1gI4H8B/AvgegDXOufOz42MBPOOcy/XVTe5ka2pq6uVpE0IAoLa2Nncc2slWkJPNOfcBgKkiMgzAMgAXhn6sJxOznGzads/edLJNnDgRQN862RoaGnJVQfrSyZbcmmk52SynkuZk05xDobEdO3bg3HPPBdD7TjbrfqS/V7t37845eq3t0aV2srW3t+caUlj30aJHYTLn3CEALwCYBWCYiPhfEGMAtBY1A0JIyeh2BReRSgDvO+cOichAAH+NjIPtjwBuAPAIgC8C+J12jdbWjO2PHj06d+yTG0JoK/Xhw4dVnXTCQJLQb9oDBw4AKC5xobOzUx3Tfjtrej5sWGzSTzFhoZCObyllreDWyqm1gLLuRyghY8eOHQDs+VurqnY/rHmk6/JVVFTk2mdZK6e1gls14LSx9Ao+ZswY1NfXA9CfCK688kr1dYDCHtGrATyY/Tv8NACPOueeFJHNAB4RkX8F8CaA+wu4FiGkjHRr4M65BgDTAvJmADNKMSlCSO/AraqERAwNnJCIoYETEjFlKdlECCk9LNlEyP8zaOCEREzJHtEJIX0PV3BCIoYGTkjElMXARWS+iGwVkW0icnc5XlOZx04R2SAi9SLyRplf+wERaRORjQnZcBFZma2Ks1JEzrKuUcJ53CMiLdn7Ui8i15R4DmNF5I8i0pitEvSNrLys98OYR7nvR+mqJjnnSvoPwOkAtgMYD6A/gPUAJpb6dZW57AQwso9e+1IA0wFsTMi+C+Du7PHdAO7to3ncA+Dvy3gvqgFMzx4PAfAWgInlvh/GPMp9PwTA4OxxPwCvIpOx+SiAL2Tl/wXg6z29djlW8BkAtjnnmp1zx5HJPru2DK/7ocI5twrAgZT4WmSq4QBlqoqjzKOsOOf2OOfWZY87ATQCqEGZ74cxj7LiMpSkalI5DLwGwK7E+W70wU3M4gCsEJG1IvLVPppDklHOuT1A5ssGoKoP57JIRBqyj/Al/1PBIyLjkElmehV9eD9S8wDKfD9E5HQRqQfQBmAlMk+9h5xzPk+0KLsph4GHkoj7KjY32zk3HcDVAP5WRC7to3l82PgpgPOQKaq5B8D3y/GiIjIYwBIAdzrn9GT/8s+j7PfDOfeBc24qMsVTZqAXqiYB5THw3QDGJs77rPqLc641+38bMqWn+jrddZ+IVANA9v+2bn6+JDjn9mW/YCcBLEYZ7ouI9EPGqH7lnFuaFZf9foTm0Rf3w+N6uWpSOQz8dQC1WY9gfwBfALC8DK+bh4gMEpEh/hjAVQA22lolZzky1XCAbqrilBJvVFmuR4nvi2RKw9wPoNE594PEUFnvhzaPPrgfldl6h0hUTWrE/1ZNAoq9H2XyEl6DjIdyO4B/LJd3MjWH8ch48NcD2FTueQB4GJnHvfeRear5MoARAJ4D0JT9f3gfzeOXADYAaCtXWs0AAABeSURBVEDGyKpLPIc5yDxuNgCoz/67ptz3w5hHue/HFGSqIjUg88vk24nv7GsAtgH4LYAzenptblUlJGK4k42QiKGBExIxNHBCIoYGTkjE0MAJiRgaOCERQwMnJGL+B2z644tAM6tNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "print('Checking first image and label in training set'); print('--'*40)\n",
    "plt.imshow(X_train[0], cmap = plt.cm.binary)    \n",
    "plt.show()\n",
    "print('Label:', y_train_o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first image and label in validation set\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "print('Checking first image and label in validation set'); print('--'*40)\n",
    "plt.imshow(X_val[0], cmap = plt.cm.binary)    \n",
    "plt.show()\n",
    "print('Label:', y_val_o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first image and label in test set\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print('Checking first image and label in test set'); print('--'*40)\n",
    "plt.imshow(X_test[0], cmap = plt.cm.binary)    \n",
    "plt.show()\n",
    "print('Label:', y_test_o[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the images for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
      "--------------------------------------------------------------------------------\n",
      "Making sure that the values are float so that we can get decimal points after division\n",
      "--------------------------------------------------------------------------------\n",
      "Normalizing the RGB codes by dividing it to the max RGB value\n",
      "--------------------------------------------------------------------------------\n",
      "Converting y data into categorical (one-hot encoding)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_val = X_val.reshape((X_val.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
    "y_train = to_categorical(y_train_o)\n",
    "y_val = to_categorical(y_val_o)\n",
    "y_test = to_categorical(y_test_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42000, 1024)\n",
      "X_val shape: (60000, 1024)\n",
      "X_test shape: (18000, 1024)\n",
      "\n",
      "\n",
      "y_train shape: (42000, 10)\n",
      "y_val shape: (60000, 10)\n",
      "y_test shape: (18000, 10)\n",
      "\n",
      "\n",
      "Number of images in X_train 42000\n",
      "Number of images in X_val 60000\n",
      "Number of images in X_test 18000\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('Number of images in X_train', X_train.shape[0])\n",
    "print('Number of images in X_val', X_val.shape[0])\n",
    "print('Number of images in X_test', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None        \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.dot(X, self.W) + self.b\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "        self.gradB = np.sum(nextgrad, axis=0)\n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradInput = nextgrad.copy()\n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()        \n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN class: To enable the forward prop and backward propagation of the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, lossfunc = CrossEntropy(), mode = 'train'):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.loss_func = lossfunc\n",
    "        self.grads = []\n",
    "        self.mode = mode\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out,y)\n",
    "        nextgrad = self.loss_func.backward(out,y)\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return p\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update function SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "        \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu = 0.9, X_val = None, y_val = None, Lambda = 0, verb = True):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        # iterate over mini batches\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        # accuracy of model at end of epoch after all mini batch updates\n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        y_train_pred = []\n",
    "        y_val_pred = []\n",
    "        y_train1 = []\n",
    "        y_vall = []\n",
    "        for ii in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[ii:ii + minibatch_size, : ]\n",
    "            y_tr = y_train[ii:ii + minibatch_size,]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "        for ii in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[ii:ii + minibatch_size, : ]\n",
    "            y_va = y_val[ii:ii + minibatch_size,]\n",
    "            y_vall = np.append(y_vall, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
    "        \n",
    "        ## weights\n",
    "        w = np.array(net.params[0][0])\n",
    "        \n",
    "        ## adding regularization to cost\n",
    "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
    "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        if verb:\n",
    "            if i%50==0:\n",
    "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
    "    return net, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "def train_and_test_loop(iterations, lr, Lambda, verb = True):\n",
    "    ## hyperparameters\n",
    "    iterations = iterations\n",
    "    learning_rate = lr\n",
    "    hidden_nodes1 = 10\n",
    "    output_nodes = 10\n",
    "\n",
    "    ## define neural net\n",
    "    nn = NN()\n",
    "    nn.add_layer(Linear(input_dim, hidden_nodes1))\n",
    "\n",
    "    nn, val_acc = train(nn, X_train, y_train_o, minibatch_size = 200, epoch = iterations, learning_rate = learning_rate,\\\n",
    "                      X_val = X_test, y_val = y_test_o, Lambda = Lambda, verb = verb)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 2.31145751021053 | Training Accuracy = 0.09942857142857142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09994444444444445"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 0\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 2.31083154726018 | Training Accuracy = 0.10211904761904762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09766666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 1e3\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting small subset of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1024), (20,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_subset = X_train[0:20]\n",
    "y_train_subset = y_train_o[0:20]\n",
    "\n",
    "X_train = X_train_subset\n",
    "y_train_o = y_train_subset\n",
    "\n",
    "X_train.shape, y_train_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Epoch 0/5000: Loss = 2.3410260505103104 | Training Accuracy = 0.0\n",
      "Epoch 50/5000: Loss = 1.9484721432438152 | Training Accuracy = 0.2\n",
      "Epoch 100/5000: Loss = 1.8629771444497294 | Training Accuracy = 0.25\n",
      "Epoch 150/5000: Loss = 1.808252871522988 | Training Accuracy = 0.4\n",
      "Epoch 200/5000: Loss = 1.7628209816084148 | Training Accuracy = 0.45\n",
      "Epoch 250/5000: Loss = 1.7219836275204412 | Training Accuracy = 0.45\n",
      "Epoch 300/5000: Loss = 1.6840509838924587 | Training Accuracy = 0.45\n",
      "Epoch 350/5000: Loss = 1.6482491522071263 | Training Accuracy = 0.45\n",
      "Epoch 400/5000: Loss = 1.6141651198456024 | Training Accuracy = 0.45\n",
      "Epoch 450/5000: Loss = 1.5815505939581838 | Training Accuracy = 0.45\n",
      "Epoch 500/5000: Loss = 1.550240485555644 | Training Accuracy = 0.5\n",
      "Epoch 550/5000: Loss = 1.5201153707222175 | Training Accuracy = 0.5\n",
      "Epoch 600/5000: Loss = 1.4910828249562003 | Training Accuracy = 0.5\n",
      "Epoch 650/5000: Loss = 1.4630675226110386 | Training Accuracy = 0.55\n",
      "Epoch 700/5000: Loss = 1.436005672614419 | Training Accuracy = 0.65\n",
      "Epoch 750/5000: Loss = 1.4098417200321083 | Training Accuracy = 0.65\n",
      "Epoch 800/5000: Loss = 1.3845262914055398 | Training Accuracy = 0.65\n",
      "Epoch 850/5000: Loss = 1.3600148543826776 | Training Accuracy = 0.65\n",
      "Epoch 900/5000: Loss = 1.336266805010368 | Training Accuracy = 0.65\n",
      "Epoch 950/5000: Loss = 1.3132448210921348 | Training Accuracy = 0.65\n",
      "Epoch 1000/5000: Loss = 1.2909143870289057 | Training Accuracy = 0.65\n",
      "Epoch 1050/5000: Loss = 1.2692434328462252 | Training Accuracy = 0.65\n",
      "Epoch 1100/5000: Loss = 1.2482020515920376 | Training Accuracy = 0.65\n",
      "Epoch 1150/5000: Loss = 1.227762272067367 | Training Accuracy = 0.65\n",
      "Epoch 1200/5000: Loss = 1.2078978716796682 | Training Accuracy = 0.7\n",
      "Epoch 1250/5000: Loss = 1.1885842191326443 | Training Accuracy = 0.7\n",
      "Epoch 1300/5000: Loss = 1.1697981398397341 | Training Accuracy = 0.75\n",
      "Epoch 1350/5000: Loss = 1.1515177990391292 | Training Accuracy = 0.75\n",
      "Epoch 1400/5000: Loss = 1.1337225989937296 | Training Accuracy = 0.75\n",
      "Epoch 1450/5000: Loss = 1.1163930876221382 | Training Accuracy = 0.75\n",
      "Epoch 1500/5000: Loss = 1.099510876577698 | Training Accuracy = 0.75\n",
      "Epoch 1550/5000: Loss = 1.083058567267678 | Training Accuracy = 0.8\n",
      "Epoch 1600/5000: Loss = 1.0670196836462744 | Training Accuracy = 0.8\n",
      "Epoch 1650/5000: Loss = 1.051378610864078 | Training Accuracy = 0.8\n",
      "Epoch 1700/5000: Loss = 1.0361205390405923 | Training Accuracy = 0.85\n",
      "Epoch 1750/5000: Loss = 1.021231411564008 | Training Accuracy = 0.85\n",
      "Epoch 1800/5000: Loss = 1.0066978774266304 | Training Accuracy = 0.85\n",
      "Epoch 1850/5000: Loss = 0.9925072471842828 | Training Accuracy = 0.85\n",
      "Epoch 1900/5000: Loss = 0.9786474521900367 | Training Accuracy = 0.85\n",
      "Epoch 1950/5000: Loss = 0.9651070068013745 | Training Accuracy = 0.85\n",
      "Epoch 2000/5000: Loss = 0.9518749732987957 | Training Accuracy = 0.85\n",
      "Epoch 2050/5000: Loss = 0.9389409292852795 | Training Accuracy = 0.85\n",
      "Epoch 2100/5000: Loss = 0.926294937361796 | Training Accuracy = 0.85\n",
      "Epoch 2150/5000: Loss = 0.9139275168955099 | Training Accuracy = 0.85\n",
      "Epoch 2200/5000: Loss = 0.9018296177154401 | Training Accuracy = 0.9\n",
      "Epoch 2250/5000: Loss = 0.8899925955858705 | Training Accuracy = 0.95\n",
      "Epoch 2300/5000: Loss = 0.8784081893212872 | Training Accuracy = 0.95\n",
      "Epoch 2350/5000: Loss = 0.8670684994184692 | Training Accuracy = 0.95\n",
      "Epoch 2400/5000: Loss = 0.8559659680918893 | Training Accuracy = 0.95\n",
      "Epoch 2450/5000: Loss = 0.8450933606080062 | Training Accuracy = 0.95\n",
      "Epoch 2500/5000: Loss = 0.8344437478225523 | Training Accuracy = 0.95\n",
      "Epoch 2550/5000: Loss = 0.8240104898326498 | Training Accuracy = 0.95\n",
      "Epoch 2600/5000: Loss = 0.8137872206626356 | Training Accuracy = 0.95\n",
      "Epoch 2650/5000: Loss = 0.8037678339089209 | Training Accuracy = 0.95\n",
      "Epoch 2700/5000: Loss = 0.7939464692751373 | Training Accuracy = 0.95\n",
      "Epoch 2750/5000: Loss = 0.7843174999342309 | Training Accuracy = 0.95\n",
      "Epoch 2800/5000: Loss = 0.7748755206591564 | Training Accuracy = 1.0\n",
      "Epoch 2850/5000: Loss = 0.7656153366684142 | Training Accuracy = 1.0\n",
      "Epoch 2900/5000: Loss = 0.7565319531368653 | Training Accuracy = 1.0\n",
      "Epoch 2950/5000: Loss = 0.7476205653261332 | Training Accuracy = 1.0\n",
      "Epoch 3000/5000: Loss = 0.738876549292458 | Training Accuracy = 1.0\n",
      "Epoch 3050/5000: Loss = 0.730295453133103 | Training Accuracy = 1.0\n",
      "Epoch 3100/5000: Loss = 0.7218729887354283 | Training Accuracy = 1.0\n",
      "Epoch 3150/5000: Loss = 0.7136050239954627 | Training Accuracy = 1.0\n",
      "Epoch 3200/5000: Loss = 0.7054875754753255 | Training Accuracy = 1.0\n",
      "Epoch 3250/5000: Loss = 0.6975168014711481 | Training Accuracy = 1.0\n",
      "Epoch 3300/5000: Loss = 0.6896889954652509 | Training Accuracy = 1.0\n",
      "Epoch 3350/5000: Loss = 0.6820005799382672 | Training Accuracy = 1.0\n",
      "Epoch 3400/5000: Loss = 0.6744481005186601 | Training Accuracy = 1.0\n",
      "Epoch 3450/5000: Loss = 0.6670282204487231 | Training Accuracy = 1.0\n",
      "Epoch 3500/5000: Loss = 0.6597377153476207 | Training Accuracy = 1.0\n",
      "Epoch 3550/5000: Loss = 0.6525734682534072 | Training Accuracy = 1.0\n",
      "Epoch 3600/5000: Loss = 0.6455324649272007 | Training Accuracy = 1.0\n",
      "Epoch 3650/5000: Loss = 0.6386117894038547 | Training Accuracy = 1.0\n",
      "Epoch 3700/5000: Loss = 0.631808619774524 | Training Accuracy = 1.0\n",
      "Epoch 3750/5000: Loss = 0.625120224187494 | Training Accuracy = 1.0\n",
      "Epoch 3800/5000: Loss = 0.6185439570545486 | Training Accuracy = 1.0\n",
      "Epoch 3850/5000: Loss = 0.6120772554509849 | Training Accuracy = 1.0\n",
      "Epoch 3900/5000: Loss = 0.6057176356981372 | Training Accuracy = 1.0\n",
      "Epoch 3950/5000: Loss = 0.5994626901179997 | Training Accuracy = 1.0\n",
      "Epoch 4000/5000: Loss = 0.5933100839501858 | Training Accuracy = 1.0\n",
      "Epoch 4050/5000: Loss = 0.5872575524220699 | Training Accuracy = 1.0\n",
      "Epoch 4100/5000: Loss = 0.5813028979635335 | Training Accuracy = 1.0\n",
      "Epoch 4150/5000: Loss = 0.5754439875582549 | Training Accuracy = 1.0\n",
      "Epoch 4200/5000: Loss = 0.5696787502239761 | Training Accuracy = 1.0\n",
      "Epoch 4250/5000: Loss = 0.5640051746146393 | Training Accuracy = 1.0\n",
      "Epoch 4300/5000: Loss = 0.5584213067377035 | Training Accuracy = 1.0\n",
      "Epoch 4350/5000: Loss = 0.5529252477803583 | Training Accuracy = 1.0\n",
      "Epoch 4400/5000: Loss = 0.5475151520387185 | Training Accuracy = 1.0\n",
      "Epoch 4450/5000: Loss = 0.5421892249444318 | Training Accuracy = 1.0\n",
      "Epoch 4500/5000: Loss = 0.5369457211834532 | Training Accuracy = 1.0\n",
      "Epoch 4550/5000: Loss = 0.5317829429020527 | Training Accuracy = 1.0\n",
      "Epoch 4600/5000: Loss = 0.5266992379954014 | Training Accuracy = 1.0\n",
      "Epoch 4650/5000: Loss = 0.5216929984743492 | Training Accuracy = 1.0\n",
      "Epoch 4700/5000: Loss = 0.5167626589062623 | Training Accuracy = 1.0\n",
      "Epoch 4750/5000: Loss = 0.5119066949260225 | Training Accuracy = 1.0\n",
      "Epoch 4800/5000: Loss = 0.507123621813509 | Training Accuracy = 1.0\n",
      "Epoch 4850/5000: Loss = 0.5024119931341012 | Training Accuracy = 1.0\n",
      "Epoch 4900/5000: Loss = 0.4977703994389227 | Training Accuracy = 1.0\n",
      "Epoch 4950/5000: Loss = 0.4931974670217426 | Training Accuracy = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13622222222222222"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "lr = 0.001\n",
    "Lambda = 0\n",
    "train_and_test_loop(5000, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
      "--------------------------------------------------------------------------------\n",
      "Making sure that the values are float so that we can get decimal points after division\n",
      "--------------------------------------------------------------------------------\n",
      "Normalizing the RGB codes by dividing it to the max RGB value\n",
      "--------------------------------------------------------------------------------\n",
      "Converting y data into categorical (one-hot encoding)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "# Load the training, validation and test sets\n",
    "X_train = h5_SVH['X_train'][:]\n",
    "y_train_o = h5_SVH['y_train'][:]\n",
    "X_val = h5_SVH['X_val'][:]\n",
    "y_val_o = h5_SVH['y_val'][:]\n",
    "X_test = h5_SVH['X_test'][:]\n",
    "y_test_o = h5_SVH['y_test'][:]\n",
    "\n",
    "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_val = X_val.reshape((X_val.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
    "y_train = to_categorical(y_train_o)\n",
    "y_val = to_categorical(y_val_o)\n",
    "y_test = to_categorical(y_test_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 2.3118413056955553 | Training Accuracy = 0.0871904761904762\n",
      "Epoch 50/500: Loss = 2.308826163772622 | Training Accuracy = 0.0864047619047619\n",
      "Epoch 100/500: Loss = 2.3068079766788525 | Training Accuracy = 0.087\n",
      "Epoch 150/500: Loss = 2.3054636893174876 | Training Accuracy = 0.08830952380952381\n",
      "Epoch 200/500: Loss = 2.3045711917313594 | Training Accuracy = 0.0889047619047619\n",
      "Epoch 250/500: Loss = 2.3039793425722417 | Training Accuracy = 0.09007142857142857\n",
      "Epoch 300/500: Loss = 2.3035862772635394 | Training Accuracy = 0.09219047619047618\n",
      "Epoch 350/500: Loss = 2.303323923229406 | Training Accuracy = 0.0929047619047619\n",
      "Epoch 400/500: Loss = 2.3031471204305785 | Training Accuracy = 0.09304761904761905\n",
      "Epoch 450/500: Loss = 2.303026092974717 | Training Accuracy = 0.09369047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09522222222222222"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 2.304271081101503 | Training Accuracy = 0.10133333333333333\n",
      "Epoch 50/500: Loss = 2.259087415222741 | Training Accuracy = 0.19452380952380952\n",
      "Epoch 100/500: Loss = 2.2506304095552445 | Training Accuracy = 0.2059047619047619\n",
      "Epoch 150/500: Loss = 2.2463310071351974 | Training Accuracy = 0.21114285714285713\n",
      "Epoch 200/500: Loss = 2.2434506448186844 | Training Accuracy = 0.21521428571428572\n",
      "Epoch 250/500: Loss = 2.241281719281938 | Training Accuracy = 0.21888095238095237\n",
      "Epoch 300/500: Loss = 2.2395416928283214 | Training Accuracy = 0.22066666666666668\n",
      "Epoch 350/500: Loss = 2.2380883803939016 | Training Accuracy = 0.2221904761904762\n",
      "Epoch 400/500: Loss = 2.2368397887387372 | Training Accuracy = 0.22357142857142856\n",
      "Epoch 450/500: Loss = 2.2357443025849273 | Training Accuracy = 0.2245952380952381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20694444444444443"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/10: Best_val_acc: 0.20072222222222222, lr: 0.003764414422989899, Lambda: 2.8312476335336574\n",
      "\n",
      "Try 2/10: Best_val_acc: 0.19416666666666665, lr: 0.007829359462566987, Lambda: 2.2323094795678966e-05\n",
      "\n",
      "Try 3/10: Best_val_acc: 0.2106111111111111, lr: 0.0028730581729452817, Lambda: 0.0038304889939502875\n",
      "\n",
      "Try 4/10: Best_val_acc: 0.2115, lr: 0.0014450887226288862, Lambda: 92.43661585102781\n",
      "\n",
      "Try 5/10: Best_val_acc: 0.20183333333333334, lr: 0.00102027647169909, Lambda: 5.238648685054541e-05\n",
      "\n",
      "Try 6/10: Best_val_acc: 0.20622222222222222, lr: 0.0016802640086022525, Lambda: 0.3469048194222294\n",
      "\n",
      "Try 7/10: Best_val_acc: 0.20277777777777778, lr: 0.007108224897130439, Lambda: 34.312418297180585\n",
      "\n",
      "Try 8/10: Best_val_acc: 0.20205555555555554, lr: 0.002106154057936919, Lambda: 0.00023780986424822543\n",
      "\n",
      "Try 9/10: Best_val_acc: 0.2066111111111111, lr: 0.004259695944877485, Lambda: 0.9546624124101437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1, 10):\n",
    "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5, 2))\n",
    "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 10, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : Best accuracy achieved using this method after hyperparameter optimization: 21%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling - Neural Network API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with sigmoid activations\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with sigmoid activations'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model1 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions sigmoid\n",
    "model1.add(Dense(128, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model1.add(Dense(64))\n",
    "# Adding activation function\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model1.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model1.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 140,106\n",
      "Trainable params: 140,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 2.3090 - accuracy: 0.0991 - val_loss: 2.3028 - val_accuracy: 0.1074\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3030 - accuracy: 0.1037 - val_loss: 2.3027 - val_accuracy: 0.1041\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3030 - accuracy: 0.1028 - val_loss: 2.3027 - val_accuracy: 0.1034\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3028 - accuracy: 0.1042 - val_loss: 2.3027 - val_accuracy: 0.0973\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 2.3028 - accuracy: 0.1035 - val_loss: 2.3028 - val_accuracy: 0.1027\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3029 - accuracy: 0.1023 - val_loss: 2.3026 - val_accuracy: 0.1058\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3028 - accuracy: 0.1044 - val_loss: 2.3026 - val_accuracy: 0.1055\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3028 - accuracy: 0.1042 - val_loss: 2.3025 - val_accuracy: 0.1081\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3027 - accuracy: 0.1031 - val_loss: 2.3024 - val_accuracy: 0.1041\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3027 - accuracy: 0.1057 - val_loss: 2.3024 - val_accuracy: 0.1063\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3026 - accuracy: 0.1043 - val_loss: 2.3024 - val_accuracy: 0.1024\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3026 - accuracy: 0.1064 - val_loss: 2.3023 - val_accuracy: 0.1051\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3026 - accuracy: 0.1049 - val_loss: 2.3023 - val_accuracy: 0.1054\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3025 - accuracy: 0.1048 - val_loss: 2.3023 - val_accuracy: 0.0964\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3025 - accuracy: 0.1050 - val_loss: 2.3023 - val_accuracy: 0.1050\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3024 - accuracy: 0.1051 - val_loss: 2.3023 - val_accuracy: 0.1087\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3024 - accuracy: 0.1073 - val_loss: 2.3022 - val_accuracy: 0.1123\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3023 - accuracy: 0.1079 - val_loss: 2.3022 - val_accuracy: 0.1013\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3023 - accuracy: 0.1098 - val_loss: 2.3021 - val_accuracy: 0.1093\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 2.3023 - accuracy: 0.1057 - val_loss: 2.3020 - val_accuracy: 0.1082\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3022 - accuracy: 0.1091 - val_loss: 2.3021 - val_accuracy: 0.1081\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3022 - accuracy: 0.1079 - val_loss: 2.3020 - val_accuracy: 0.1109\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3022 - accuracy: 0.1119 - val_loss: 2.3021 - val_accuracy: 0.1082\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3020 - accuracy: 0.1068 - val_loss: 2.3020 - val_accuracy: 0.1113\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3021 - accuracy: 0.1129 - val_loss: 2.3019 - val_accuracy: 0.1066\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3021 - accuracy: 0.1089 - val_loss: 2.3019 - val_accuracy: 0.1102\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 2.3020 - accuracy: 0.1128 - val_loss: 2.3019 - val_accuracy: 0.1131\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3020 - accuracy: 0.1106 - val_loss: 2.3020 - val_accuracy: 0.1118\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3020 - accuracy: 0.1133 - val_loss: 2.3019 - val_accuracy: 0.1067\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3019 - accuracy: 0.1087 - val_loss: 2.3018 - val_accuracy: 0.1164\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3019 - accuracy: 0.1111 - val_loss: 2.3017 - val_accuracy: 0.1089\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3018 - accuracy: 0.1130 - val_loss: 2.3017 - val_accuracy: 0.1115\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3018 - accuracy: 0.1096 - val_loss: 2.3016 - val_accuracy: 0.1178\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 4s 93us/sample - loss: 2.3017 - accuracy: 0.1138 - val_loss: 2.3017 - val_accuracy: 0.1131\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3017 - accuracy: 0.1152 - val_loss: 2.3016 - val_accuracy: 0.1106\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3017 - accuracy: 0.1136 - val_loss: 2.3016 - val_accuracy: 0.1120\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3017 - accuracy: 0.1102 - val_loss: 2.3015 - val_accuracy: 0.1216\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3016 - accuracy: 0.1135 - val_loss: 2.3016 - val_accuracy: 0.1087\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3015 - accuracy: 0.1127 - val_loss: 2.3016 - val_accuracy: 0.1118\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3016 - accuracy: 0.1140 - val_loss: 2.3014 - val_accuracy: 0.1218\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3015 - accuracy: 0.1174 - val_loss: 2.3013 - val_accuracy: 0.1191\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3015 - accuracy: 0.1167 - val_loss: 2.3013 - val_accuracy: 0.1136\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3014 - accuracy: 0.1155 - val_loss: 2.3014 - val_accuracy: 0.1106\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3015 - accuracy: 0.1169 - val_loss: 2.3012 - val_accuracy: 0.1208\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.3013 - accuracy: 0.1136 - val_loss: 2.3013 - val_accuracy: 0.1104\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3013 - accuracy: 0.1141 - val_loss: 2.3013 - val_accuracy: 0.1125\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3014 - accuracy: 0.1156 - val_loss: 2.3011 - val_accuracy: 0.1263\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3013 - val_accuracy: 0.1157\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3012 - accuracy: 0.1195 - val_loss: 2.3011 - val_accuracy: 0.1145\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3012 - accuracy: 0.1171 - val_loss: 2.3011 - val_accuracy: 0.1093\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3012 - accuracy: 0.1155 - val_loss: 2.3010 - val_accuracy: 0.1274\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3011 - accuracy: 0.1191 - val_loss: 2.3010 - val_accuracy: 0.1144\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3011 - accuracy: 0.1154 - val_loss: 2.3011 - val_accuracy: 0.1179\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3011 - accuracy: 0.1213 - val_loss: 2.3009 - val_accuracy: 0.1223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3010 - accuracy: 0.1188 - val_loss: 2.3009 - val_accuracy: 0.1175\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3010 - accuracy: 0.1224 - val_loss: 2.3008 - val_accuracy: 0.1257\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3009 - accuracy: 0.1185 - val_loss: 2.3010 - val_accuracy: 0.1170\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3009 - accuracy: 0.1191 - val_loss: 2.3008 - val_accuracy: 0.1192\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3008 - accuracy: 0.1196 - val_loss: 2.3010 - val_accuracy: 0.1076\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3008 - accuracy: 0.1178 - val_loss: 2.3009 - val_accuracy: 0.1196\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3009 - accuracy: 0.1192 - val_loss: 2.3007 - val_accuracy: 0.1238\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3007 - accuracy: 0.1192 - val_loss: 2.3007 - val_accuracy: 0.1292\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3007 - accuracy: 0.1218 - val_loss: 2.3007 - val_accuracy: 0.1279\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3007 - accuracy: 0.1217 - val_loss: 2.3006 - val_accuracy: 0.1097\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.3007 - accuracy: 0.1164 - val_loss: 2.3006 - val_accuracy: 0.1250\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.3006 - accuracy: 0.1192 - val_loss: 2.3006 - val_accuracy: 0.1297\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3006 - accuracy: 0.1212 - val_loss: 2.3005 - val_accuracy: 0.1274\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3006 - accuracy: 0.1236 - val_loss: 2.3004 - val_accuracy: 0.1208\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3005 - accuracy: 0.1249 - val_loss: 2.3006 - val_accuracy: 0.1187\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3004 - accuracy: 0.1223 - val_loss: 2.3005 - val_accuracy: 0.1185\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3004 - accuracy: 0.1215 - val_loss: 2.3006 - val_accuracy: 0.1212\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3005 - accuracy: 0.1242 - val_loss: 2.3003 - val_accuracy: 0.1387\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.3004 - accuracy: 0.1279 - val_loss: 2.3004 - val_accuracy: 0.1208\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3003 - accuracy: 0.1224 - val_loss: 2.3004 - val_accuracy: 0.1238\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 4s 93us/sample - loss: 2.3003 - accuracy: 0.1262 - val_loss: 2.3002 - val_accuracy: 0.1239\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 2.3002 - accuracy: 0.1232 - val_loss: 2.3004 - val_accuracy: 0.1239\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3002 - accuracy: 0.1231 - val_loss: 2.3003 - val_accuracy: 0.1212\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3002 - accuracy: 0.1266 - val_loss: 2.3003 - val_accuracy: 0.1197\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.3002 - accuracy: 0.1264 - val_loss: 2.3001 - val_accuracy: 0.1365\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 2.3001 - accuracy: 0.1260 - val_loss: 2.3000 - val_accuracy: 0.1316\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 2.3001 - accuracy: 0.1255 - val_loss: 2.3000 - val_accuracy: 0.1317\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3001 - accuracy: 0.1282 - val_loss: 2.2999 - val_accuracy: 0.1326\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 2.3000 - accuracy: 0.1295 - val_loss: 2.2999 - val_accuracy: 0.1255\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.3000 - accuracy: 0.1302 - val_loss: 2.2998 - val_accuracy: 0.1365\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2999 - accuracy: 0.1326 - val_loss: 2.2999 - val_accuracy: 0.1199\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2998 - accuracy: 0.1252 - val_loss: 2.2999 - val_accuracy: 0.1137\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2998 - accuracy: 0.1229 - val_loss: 2.2998 - val_accuracy: 0.1355\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2998 - accuracy: 0.1286 - val_loss: 2.2997 - val_accuracy: 0.1371\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.2998 - accuracy: 0.1276 - val_loss: 2.2999 - val_accuracy: 0.1155\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2997 - accuracy: 0.1260 - val_loss: 2.2997 - val_accuracy: 0.1305\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2998 - accuracy: 0.1319 - val_loss: 2.2997 - val_accuracy: 0.1399\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2997 - accuracy: 0.1313 - val_loss: 2.2995 - val_accuracy: 0.1357\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2995 - accuracy: 0.1317 - val_loss: 2.2996 - val_accuracy: 0.1284\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.2996 - accuracy: 0.1344 - val_loss: 2.2995 - val_accuracy: 0.1381\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2996 - accuracy: 0.1326 - val_loss: 2.2994 - val_accuracy: 0.1377\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2995 - accuracy: 0.1347 - val_loss: 2.2994 - val_accuracy: 0.1309\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2994 - accuracy: 0.1315 - val_loss: 2.2994 - val_accuracy: 0.1298\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2994 - accuracy: 0.1297 - val_loss: 2.2993 - val_accuracy: 0.1344\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2993 - accuracy: 0.1336 - val_loss: 2.2994 - val_accuracy: 0.1400\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2994 - accuracy: 0.1321 - val_loss: 2.2993 - val_accuracy: 0.1463\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with sigmoid activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 2.2993 - accuracy: 0.1463\n",
      "Validation accuracy: 14.63\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with sigmoid activations'); print('--'*40)\n",
    "results1 = model1.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, sigmoid activations, SGD optimizer, changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with sigmoid activations - changing learning rate\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 10s 240us/sample - loss: 2.2991 - accuracy: 0.1475 - val_loss: 2.2992 - val_accuracy: 0.1488\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2990 - accuracy: 0.1502 - val_loss: 2.2992 - val_accuracy: 0.1490\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2990 - accuracy: 0.1495 - val_loss: 2.2992 - val_accuracy: 0.1479\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2990 - accuracy: 0.1463 - val_loss: 2.2992 - val_accuracy: 0.1469\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2990 - accuracy: 0.1467 - val_loss: 2.2991 - val_accuracy: 0.1463\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2990 - accuracy: 0.1482 - val_loss: 2.2991 - val_accuracy: 0.1441\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2990 - accuracy: 0.1442 - val_loss: 2.2991 - val_accuracy: 0.1444\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2990 - accuracy: 0.1434 - val_loss: 2.2991 - val_accuracy: 0.1441\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2990 - accuracy: 0.1450 - val_loss: 2.2991 - val_accuracy: 0.1440\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2990 - accuracy: 0.1467 - val_loss: 2.2991 - val_accuracy: 0.1427\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2990 - accuracy: 0.1419 - val_loss: 2.2991 - val_accuracy: 0.1434\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2990 - accuracy: 0.1435 - val_loss: 2.2991 - val_accuracy: 0.1435\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2990 - accuracy: 0.1458 - val_loss: 2.2991 - val_accuracy: 0.1424\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2990 - accuracy: 0.1439 - val_loss: 2.2991 - val_accuracy: 0.1429\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2990 - accuracy: 0.1420 - val_loss: 2.2991 - val_accuracy: 0.1446\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2990 - accuracy: 0.1441 - val_loss: 2.2991 - val_accuracy: 0.1440\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.2989 - accuracy: 0.1458 - val_loss: 2.2991 - val_accuracy: 0.1432\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1431 - val_loss: 2.2991 - val_accuracy: 0.1441\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2989 - accuracy: 0.1462 - val_loss: 2.2991 - val_accuracy: 0.1434\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1437 - val_loss: 2.2991 - val_accuracy: 0.1442\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2989 - accuracy: 0.1421 - val_loss: 2.2991 - val_accuracy: 0.1449\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2989 - accuracy: 0.1449 - val_loss: 2.2991 - val_accuracy: 0.1438\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2989 - accuracy: 0.1440 - val_loss: 2.2991 - val_accuracy: 0.1443\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1455 - val_loss: 2.2991 - val_accuracy: 0.1437\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1448 - val_loss: 2.2991 - val_accuracy: 0.1437\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1449 - val_loss: 2.2991 - val_accuracy: 0.1435\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2989 - accuracy: 0.1441 - val_loss: 2.2991 - val_accuracy: 0.1438\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2989 - accuracy: 0.1465 - val_loss: 2.2991 - val_accuracy: 0.1436\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1470 - val_loss: 2.2991 - val_accuracy: 0.1426\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1433 - val_loss: 2.2990 - val_accuracy: 0.1437\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1439 - val_loss: 2.2990 - val_accuracy: 0.1440\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2989 - accuracy: 0.1443 - val_loss: 2.2990 - val_accuracy: 0.1439\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.2989 - accuracy: 0.1440 - val_loss: 2.2990 - val_accuracy: 0.1442\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2989 - accuracy: 0.1447 - val_loss: 2.2990 - val_accuracy: 0.1447\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2989 - accuracy: 0.1427 - val_loss: 2.2990 - val_accuracy: 0.1452\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2989 - accuracy: 0.1456 - val_loss: 2.2990 - val_accuracy: 0.1452\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.2989 - accuracy: 0.1485 - val_loss: 2.2990 - val_accuracy: 0.1434\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2989 - accuracy: 0.1441 - val_loss: 2.2990 - val_accuracy: 0.1431\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1427 - val_loss: 2.2990 - val_accuracy: 0.1454\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1449 - val_loss: 2.2990 - val_accuracy: 0.1452\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2988 - accuracy: 0.1465 - val_loss: 2.2990 - val_accuracy: 0.1449\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2988 - accuracy: 0.1457 - val_loss: 2.2990 - val_accuracy: 0.1448\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2988 - accuracy: 0.1479 - val_loss: 2.2990 - val_accuracy: 0.1446\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1453 - val_loss: 2.2990 - val_accuracy: 0.1443\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1448 - val_loss: 2.2990 - val_accuracy: 0.1446\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1447 - val_loss: 2.2990 - val_accuracy: 0.1452\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1460 - val_loss: 2.2990 - val_accuracy: 0.1446\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 2.2988 - accuracy: 0.1450 - val_loss: 2.2990 - val_accuracy: 0.1449\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1444 - val_loss: 2.2990 - val_accuracy: 0.1454\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2988 - accuracy: 0.1449 - val_loss: 2.2990 - val_accuracy: 0.1447\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2988 - accuracy: 0.1430 - val_loss: 2.2990 - val_accuracy: 0.1458\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2988 - accuracy: 0.1482 - val_loss: 2.2990 - val_accuracy: 0.1448\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2988 - accuracy: 0.1432 - val_loss: 2.2989 - val_accuracy: 0.1460\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2988 - accuracy: 0.1479 - val_loss: 2.2989 - val_accuracy: 0.1452\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1475 - val_loss: 2.2989 - val_accuracy: 0.1445\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2988 - accuracy: 0.1464 - val_loss: 2.2989 - val_accuracy: 0.1447\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1457 - val_loss: 2.2989 - val_accuracy: 0.1449\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2988 - accuracy: 0.1470 - val_loss: 2.2989 - val_accuracy: 0.1445\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2988 - accuracy: 0.1471 - val_loss: 2.2989 - val_accuracy: 0.1440\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2988 - accuracy: 0.1436 - val_loss: 2.2989 - val_accuracy: 0.1444\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2988 - accuracy: 0.1452 - val_loss: 2.2989 - val_accuracy: 0.1448\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2988 - accuracy: 0.1445 - val_loss: 2.2989 - val_accuracy: 0.1450\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2987 - accuracy: 0.1447 - val_loss: 2.2989 - val_accuracy: 0.1455\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1480 - val_loss: 2.2989 - val_accuracy: 0.1450\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2987 - accuracy: 0.1447 - val_loss: 2.2989 - val_accuracy: 0.1458\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2987 - accuracy: 0.1450 - val_loss: 2.2989 - val_accuracy: 0.1460\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2987 - accuracy: 0.1439 - val_loss: 2.2989 - val_accuracy: 0.1468\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1474 - val_loss: 2.2989 - val_accuracy: 0.1461\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1487 - val_loss: 2.2989 - val_accuracy: 0.1455\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2987 - accuracy: 0.1462 - val_loss: 2.2989 - val_accuracy: 0.1455\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2987 - accuracy: 0.1447 - val_loss: 2.2989 - val_accuracy: 0.1460\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2987 - accuracy: 0.1451 - val_loss: 2.2989 - val_accuracy: 0.1470\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2987 - accuracy: 0.1473 - val_loss: 2.2989 - val_accuracy: 0.1461\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 2.2987 - accuracy: 0.1493 - val_loss: 2.2989 - val_accuracy: 0.1453\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2987 - accuracy: 0.1472 - val_loss: 2.2989 - val_accuracy: 0.1454\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1458 - val_loss: 2.2988 - val_accuracy: 0.1457\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2987 - accuracy: 0.1466 - val_loss: 2.2988 - val_accuracy: 0.1458\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1484 - val_loss: 2.2988 - val_accuracy: 0.1452\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1464 - val_loss: 2.2988 - val_accuracy: 0.1458\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2987 - accuracy: 0.1458 - val_loss: 2.2988 - val_accuracy: 0.1454\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2987 - accuracy: 0.1477 - val_loss: 2.2988 - val_accuracy: 0.1453\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2987 - accuracy: 0.1466 - val_loss: 2.2988 - val_accuracy: 0.1450\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2987 - accuracy: 0.1460 - val_loss: 2.2988 - val_accuracy: 0.1444\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2986 - accuracy: 0.1461 - val_loss: 2.2988 - val_accuracy: 0.1451\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2986 - accuracy: 0.1456 - val_loss: 2.2988 - val_accuracy: 0.1458\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2986 - accuracy: 0.1473 - val_loss: 2.2988 - val_accuracy: 0.1461\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2986 - accuracy: 0.1472 - val_loss: 2.2988 - val_accuracy: 0.1451\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2986 - accuracy: 0.1464 - val_loss: 2.2988 - val_accuracy: 0.1453\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 2.2986 - accuracy: 0.1460 - val_loss: 2.2988 - val_accuracy: 0.1454\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2986 - accuracy: 0.1471 - val_loss: 2.2988 - val_accuracy: 0.1453\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2986 - accuracy: 0.1465 - val_loss: 2.2988 - val_accuracy: 0.1457\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2986 - accuracy: 0.1484 - val_loss: 2.2988 - val_accuracy: 0.1451\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2986 - accuracy: 0.1451 - val_loss: 2.2988 - val_accuracy: 0.1459\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 2.2986 - accuracy: 0.1500 - val_loss: 2.2988 - val_accuracy: 0.1444\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2986 - accuracy: 0.1453 - val_loss: 2.2988 - val_accuracy: 0.1451\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2986 - accuracy: 0.1463 - val_loss: 2.2988 - val_accuracy: 0.1460\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2986 - accuracy: 0.1485 - val_loss: 2.2988 - val_accuracy: 0.1455\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2986 - accuracy: 0.1460 - val_loss: 2.2988 - val_accuracy: 0.1460\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 2.2986 - accuracy: 0.1456 - val_loss: 2.2987 - val_accuracy: 0.1466\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2986 - accuracy: 0.1481 - val_loss: 2.2987 - val_accuracy: 0.1454\n"
     ]
    }
   ],
   "source": [
    "print('NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.001)\n",
    "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with sigmoid activations - changing learning rate\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 2.2987 - accuracy: 0.1454\n",
      "Validation accuracy: 14.54\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
    "results1 = model1.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Validation score is very low, changing learning rate further reduces it.\n",
    "Optimizing the network in order to better learn the patterns in the dataset.\n",
    "Best model is the one with lower learning rate using SGD optimizer and sigmoid activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "NN model with relu activations and sgd optimizers\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and sgd optimizers'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model2 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu\n",
    "model2.add(Dense(128, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model2.add(Dense(64))\n",
    "# Adding activation function\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model2.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 140,106\n",
      "Trainable params: 140,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 2.3068 - accuracy: 0.1071 - val_loss: 2.3000 - val_accuracy: 0.1090\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.2961 - accuracy: 0.1207 - val_loss: 2.2924 - val_accuracy: 0.1205\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2889 - accuracy: 0.1326 - val_loss: 2.2853 - val_accuracy: 0.1471\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 2.2811 - accuracy: 0.1535 - val_loss: 2.2767 - val_accuracy: 0.1526\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 2.2708 - accuracy: 0.1768 - val_loss: 2.2652 - val_accuracy: 0.1892\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 2.2587 - accuracy: 0.1998 - val_loss: 2.2512 - val_accuracy: 0.2151\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 2.2445 - accuracy: 0.2214 - val_loss: 2.2381 - val_accuracy: 0.2157\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.2271 - accuracy: 0.2453 - val_loss: 2.2164 - val_accuracy: 0.2514\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 2.2054 - accuracy: 0.2659 - val_loss: 2.1912 - val_accuracy: 0.2972\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 2.1780 - accuracy: 0.2934 - val_loss: 2.1611 - val_accuracy: 0.3171\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 2.1426 - accuracy: 0.3238 - val_loss: 2.1223 - val_accuracy: 0.3573\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 2.0997 - accuracy: 0.3535 - val_loss: 2.0751 - val_accuracy: 0.3648\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 2.0488 - accuracy: 0.3797 - val_loss: 2.0193 - val_accuracy: 0.3940\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.9883 - accuracy: 0.3973 - val_loss: 1.9536 - val_accuracy: 0.4200\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.9227 - accuracy: 0.4182 - val_loss: 1.8897 - val_accuracy: 0.4254\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.8548 - accuracy: 0.4392 - val_loss: 1.8185 - val_accuracy: 0.4479\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.7877 - accuracy: 0.4617 - val_loss: 1.7493 - val_accuracy: 0.4776\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.7235 - accuracy: 0.4822 - val_loss: 1.6935 - val_accuracy: 0.4781\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.6636 - accuracy: 0.5026 - val_loss: 1.6347 - val_accuracy: 0.4905\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.6094 - accuracy: 0.5189 - val_loss: 1.5803 - val_accuracy: 0.5274\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 1.5586 - accuracy: 0.5349 - val_loss: 1.5338 - val_accuracy: 0.5377\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 1.5126 - accuracy: 0.5495 - val_loss: 1.5197 - val_accuracy: 0.5357\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.4700 - accuracy: 0.5603 - val_loss: 1.4509 - val_accuracy: 0.5695\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 1.4289 - accuracy: 0.5737 - val_loss: 1.4112 - val_accuracy: 0.5773\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 1.4012 - accuracy: 0.5816 - val_loss: 1.3809 - val_accuracy: 0.5834\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.3697 - accuracy: 0.5902 - val_loss: 1.3346 - val_accuracy: 0.6076\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.3407 - accuracy: 0.5977 - val_loss: 1.3387 - val_accuracy: 0.5895\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.3148 - accuracy: 0.6048 - val_loss: 1.2970 - val_accuracy: 0.6108\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.2908 - accuracy: 0.6125 - val_loss: 1.2552 - val_accuracy: 0.6302\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.2657 - accuracy: 0.6186 - val_loss: 1.2637 - val_accuracy: 0.6178\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 1.2477 - accuracy: 0.6235 - val_loss: 1.2171 - val_accuracy: 0.6378\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.2297 - accuracy: 0.6288 - val_loss: 1.2034 - val_accuracy: 0.6407\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.2066 - accuracy: 0.6393 - val_loss: 1.1779 - val_accuracy: 0.6524\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.1896 - accuracy: 0.6434 - val_loss: 1.1764 - val_accuracy: 0.6482\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.1758 - accuracy: 0.6455 - val_loss: 1.1829 - val_accuracy: 0.6369\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 1.1532 - accuracy: 0.6528 - val_loss: 1.1629 - val_accuracy: 0.6408\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.1417 - accuracy: 0.6570 - val_loss: 1.1197 - val_accuracy: 0.6678\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.1272 - accuracy: 0.6617 - val_loss: 1.1100 - val_accuracy: 0.6623\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 1.1106 - accuracy: 0.6672 - val_loss: 1.1263 - val_accuracy: 0.6558\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0954 - accuracy: 0.6707 - val_loss: 1.0788 - val_accuracy: 0.6794\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0889 - accuracy: 0.6750 - val_loss: 1.0618 - val_accuracy: 0.6857\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0778 - accuracy: 0.6747 - val_loss: 1.0661 - val_accuracy: 0.6823\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 1.0662 - accuracy: 0.6803 - val_loss: 1.0391 - val_accuracy: 0.6928\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.0532 - accuracy: 0.6846 - val_loss: 1.0446 - val_accuracy: 0.6823\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.0409 - accuracy: 0.6870 - val_loss: 1.0425 - val_accuracy: 0.6865\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0283 - accuracy: 0.6928 - val_loss: 1.0480 - val_accuracy: 0.6855\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0213 - accuracy: 0.6954 - val_loss: 1.0057 - val_accuracy: 0.7016\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.0114 - accuracy: 0.6985 - val_loss: 1.0038 - val_accuracy: 0.7005\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 1.0032 - accuracy: 0.6995 - val_loss: 1.0480 - val_accuracy: 0.6791\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.9920 - accuracy: 0.7042 - val_loss: 0.9754 - val_accuracy: 0.7100\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9825 - accuracy: 0.7056 - val_loss: 0.9705 - val_accuracy: 0.7130\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9772 - accuracy: 0.7078 - val_loss: 0.9918 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9688 - accuracy: 0.7092 - val_loss: 0.9530 - val_accuracy: 0.7183\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.9585 - accuracy: 0.7133 - val_loss: 1.0178 - val_accuracy: 0.6835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.9539 - accuracy: 0.7154 - val_loss: 0.9628 - val_accuracy: 0.7147\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.9448 - accuracy: 0.7178 - val_loss: 0.9245 - val_accuracy: 0.7279\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9356 - accuracy: 0.7221 - val_loss: 0.9312 - val_accuracy: 0.7230\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9304 - accuracy: 0.7213 - val_loss: 0.9432 - val_accuracy: 0.7179\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.9217 - accuracy: 0.7244 - val_loss: 0.9177 - val_accuracy: 0.7270\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.9138 - accuracy: 0.7289 - val_loss: 0.9277 - val_accuracy: 0.7217\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.9074 - accuracy: 0.7290 - val_loss: 0.8960 - val_accuracy: 0.7345\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9013 - accuracy: 0.7298 - val_loss: 0.8900 - val_accuracy: 0.7375\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8923 - accuracy: 0.7317 - val_loss: 0.8829 - val_accuracy: 0.7396\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8894 - accuracy: 0.7333 - val_loss: 0.8946 - val_accuracy: 0.7333loss: 0.8907 - accuracy\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8787 - accuracy: 0.7389 - val_loss: 0.8793 - val_accuracy: 0.7406\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8765 - accuracy: 0.7378 - val_loss: 0.8684 - val_accuracy: 0.7431\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.8721 - accuracy: 0.7391 - val_loss: 0.8962 - val_accuracy: 0.7317\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8614 - accuracy: 0.7449 - val_loss: 0.8610 - val_accuracy: 0.7458\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8597 - accuracy: 0.7428 - val_loss: 0.8671 - val_accuracy: 0.7411\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.8520 - accuracy: 0.7473 - val_loss: 0.8629 - val_accuracy: 0.7419\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8499 - accuracy: 0.7470 - val_loss: 0.9072 - val_accuracy: 0.7234\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8424 - accuracy: 0.7496 - val_loss: 0.8438 - val_accuracy: 0.7532\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.8397 - accuracy: 0.7489 - val_loss: 0.8227 - val_accuracy: 0.7570\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.8401 - accuracy: 0.7498 - val_loss: 0.8364 - val_accuracy: 0.7510\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8288 - accuracy: 0.7529 - val_loss: 0.8530 - val_accuracy: 0.7468\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.8226 - accuracy: 0.7546 - val_loss: 0.8073 - val_accuracy: 0.7621\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8136 - accuracy: 0.7588 - val_loss: 0.8706 - val_accuracy: 0.7381\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8145 - accuracy: 0.7569 - val_loss: 0.8841 - val_accuracy: 0.7332\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8122 - accuracy: 0.7588 - val_loss: 0.8220 - val_accuracy: 0.7557\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8004 - accuracy: 0.7623 - val_loss: 0.8210 - val_accuracy: 0.7574\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.8005 - accuracy: 0.7602 - val_loss: 0.8180 - val_accuracy: 0.7596\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.7939 - accuracy: 0.7642 - val_loss: 0.7927 - val_accuracy: 0.7652\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.7864 - accuracy: 0.7655 - val_loss: 0.8005 - val_accuracy: 0.7645\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.7864 - accuracy: 0.7660 - val_loss: 0.7854 - val_accuracy: 0.7672\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.7786 - accuracy: 0.7680 - val_loss: 0.7792 - val_accuracy: 0.7701\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.7779 - accuracy: 0.7691 - val_loss: 0.7868 - val_accuracy: 0.7686\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.7744 - accuracy: 0.7702 - val_loss: 0.7751 - val_accuracy: 0.7688\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.7689 - accuracy: 0.7727 - val_loss: 0.7685 - val_accuracy: 0.7740\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.7674 - accuracy: 0.7709 - val_loss: 0.8126 - val_accuracy: 0.7551\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7626 - accuracy: 0.7716 - val_loss: 0.7946 - val_accuracy: 0.7617\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.7600 - accuracy: 0.7730 - val_loss: 0.7714 - val_accuracy: 0.7685\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.7550 - accuracy: 0.7759 - val_loss: 0.7636 - val_accuracy: 0.7764\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.7485 - accuracy: 0.7772 - val_loss: 0.7734 - val_accuracy: 0.7697\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.7481 - accuracy: 0.7767 - val_loss: 0.7425 - val_accuracy: 0.7821\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.7407 - accuracy: 0.7806 - val_loss: 0.7376 - val_accuracy: 0.7812\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7416 - accuracy: 0.7799 - val_loss: 0.7954 - val_accuracy: 0.7584\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7344 - accuracy: 0.7819 - val_loss: 0.7330 - val_accuracy: 0.7835\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7311 - accuracy: 0.7836 - val_loss: 0.7687 - val_accuracy: 0.7735\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7278 - accuracy: 0.7840 - val_loss: 0.7295 - val_accuracy: 0.7882\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.7264 - accuracy: 0.7843 - val_loss: 0.7448 - val_accuracy: 0.7796\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.7448 - accuracy: 0.7796\n",
      "Validation accuracy: 77.96\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, SGD optimizer, changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "NN model with relu activations and sgd optimizers - changing learning rate\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 0.6828 - accuracy: 0.8002 - val_loss: 0.7054 - val_accuracy: 0.7933\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6808 - accuracy: 0.8002 - val_loss: 0.7026 - val_accuracy: 0.7959\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6796 - accuracy: 0.8020 - val_loss: 0.7025 - val_accuracy: 0.7963\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6793 - accuracy: 0.8020 - val_loss: 0.7016 - val_accuracy: 0.7953\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6788 - accuracy: 0.8012 - val_loss: 0.7019 - val_accuracy: 0.7963\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6784 - accuracy: 0.8013 - val_loss: 0.7020 - val_accuracy: 0.7957\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6781 - accuracy: 0.8019 - val_loss: 0.7005 - val_accuracy: 0.7962\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6776 - accuracy: 0.8019 - val_loss: 0.7001 - val_accuracy: 0.7962\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6771 - accuracy: 0.8020 - val_loss: 0.7001 - val_accuracy: 0.7961\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6768 - accuracy: 0.8017 - val_loss: 0.7003 - val_accuracy: 0.7963\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6762 - accuracy: 0.8026 - val_loss: 0.6988 - val_accuracy: 0.7963\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6760 - accuracy: 0.8014 - val_loss: 0.6989 - val_accuracy: 0.7970\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6753 - accuracy: 0.8022 - val_loss: 0.6995 - val_accuracy: 0.7968\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6750 - accuracy: 0.8031 - val_loss: 0.6980 - val_accuracy: 0.7969\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6746 - accuracy: 0.8034 - val_loss: 0.6982 - val_accuracy: 0.7975\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6742 - accuracy: 0.8028 - val_loss: 0.6978 - val_accuracy: 0.7966\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6739 - accuracy: 0.8017 - val_loss: 0.6975 - val_accuracy: 0.7971\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6734 - accuracy: 0.8033 - val_loss: 0.6984 - val_accuracy: 0.7962\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6732 - accuracy: 0.8035 - val_loss: 0.6966 - val_accuracy: 0.7978\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6728 - accuracy: 0.8032 - val_loss: 0.6956 - val_accuracy: 0.7981\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6724 - accuracy: 0.8033 - val_loss: 0.6959 - val_accuracy: 0.7976\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6718 - accuracy: 0.8035 - val_loss: 0.6956 - val_accuracy: 0.7978\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6716 - accuracy: 0.8035 - val_loss: 0.6953 - val_accuracy: 0.7980\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6711 - accuracy: 0.8037 - val_loss: 0.6954 - val_accuracy: 0.7978\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6708 - accuracy: 0.8037 - val_loss: 0.6947 - val_accuracy: 0.7972\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6704 - accuracy: 0.8038 - val_loss: 0.6937 - val_accuracy: 0.7983\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6697 - accuracy: 0.8044 - val_loss: 0.6939 - val_accuracy: 0.7986\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6697 - accuracy: 0.8048 - val_loss: 0.6933 - val_accuracy: 0.7988\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6691 - accuracy: 0.8045 - val_loss: 0.6931 - val_accuracy: 0.7980\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6689 - accuracy: 0.8044 - val_loss: 0.6935 - val_accuracy: 0.7986\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6684 - accuracy: 0.8057 - val_loss: 0.6920 - val_accuracy: 0.7988\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6680 - accuracy: 0.8047 - val_loss: 0.6930 - val_accuracy: 0.7978\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6678 - accuracy: 0.8055 - val_loss: 0.6913 - val_accuracy: 0.7990\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6672 - accuracy: 0.8060 - val_loss: 0.6920 - val_accuracy: 0.7983\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6667 - accuracy: 0.8050 - val_loss: 0.6908 - val_accuracy: 0.7993\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.6663 - accuracy: 0.8049 - val_loss: 0.6902 - val_accuracy: 0.7996\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6663 - accuracy: 0.8056 - val_loss: 0.6897 - val_accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.6656 - accuracy: 0.8052 - val_loss: 0.6906 - val_accuracy: 0.7994\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6657 - accuracy: 0.8057 - val_loss: 0.6889 - val_accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6649 - accuracy: 0.8060 - val_loss: 0.6901 - val_accuracy: 0.7999\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6649 - accuracy: 0.8058 - val_loss: 0.6890 - val_accuracy: 0.7999\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.6644 - accuracy: 0.8057 - val_loss: 0.6888 - val_accuracy: 0.8001\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.6642 - accuracy: 0.8057 - val_loss: 0.6883 - val_accuracy: 0.7997\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6639 - accuracy: 0.8061 - val_loss: 0.6874 - val_accuracy: 0.7998\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6632 - accuracy: 0.8062 - val_loss: 0.6896 - val_accuracy: 0.8001\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6632 - accuracy: 0.8067 - val_loss: 0.6872 - val_accuracy: 0.8004\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6627 - accuracy: 0.8064 - val_loss: 0.6866 - val_accuracy: 0.8003\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.6623 - accuracy: 0.8057 - val_loss: 0.6872 - val_accuracy: 0.8005\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6620 - accuracy: 0.8070 - val_loss: 0.6859 - val_accuracy: 0.8008\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6613 - accuracy: 0.8069 - val_loss: 0.6855 - val_accuracy: 0.8006\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6612 - accuracy: 0.8072 - val_loss: 0.6850 - val_accuracy: 0.8012\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6608 - accuracy: 0.8067 - val_loss: 0.6851 - val_accuracy: 0.8014\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6602 - accuracy: 0.8066 - val_loss: 0.6851 - val_accuracy: 0.8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6600 - accuracy: 0.8070 - val_loss: 0.6844 - val_accuracy: 0.8007\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6599 - accuracy: 0.8078 - val_loss: 0.6843 - val_accuracy: 0.8018\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6592 - accuracy: 0.8072 - val_loss: 0.6841 - val_accuracy: 0.8016\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6591 - accuracy: 0.8076 - val_loss: 0.6829 - val_accuracy: 0.8012\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6584 - accuracy: 0.8084 - val_loss: 0.6845 - val_accuracy: 0.8017\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6584 - accuracy: 0.8068 - val_loss: 0.6829 - val_accuracy: 0.8025\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6577 - accuracy: 0.8075 - val_loss: 0.6820 - val_accuracy: 0.8020\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6575 - accuracy: 0.8082 - val_loss: 0.6818 - val_accuracy: 0.8023\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6571 - accuracy: 0.8080 - val_loss: 0.6821 - val_accuracy: 0.8023\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.6567 - accuracy: 0.8080 - val_loss: 0.6812 - val_accuracy: 0.8019\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 0.6565 - accuracy: 0.8085 - val_loss: 0.6815 - val_accuracy: 0.8023\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6561 - accuracy: 0.8081 - val_loss: 0.6807 - val_accuracy: 0.8023\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6557 - accuracy: 0.8086 - val_loss: 0.6810 - val_accuracy: 0.8022\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6554 - accuracy: 0.8089 - val_loss: 0.6802 - val_accuracy: 0.8028\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6552 - accuracy: 0.8088 - val_loss: 0.6796 - val_accuracy: 0.8028\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.6545 - accuracy: 0.8100 - val_loss: 0.6799 - val_accuracy: 0.8029\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6544 - accuracy: 0.8091 - val_loss: 0.6818 - val_accuracy: 0.8015\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6541 - accuracy: 0.8091 - val_loss: 0.6791 - val_accuracy: 0.8031\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6535 - accuracy: 0.8094 - val_loss: 0.6787 - val_accuracy: 0.8022\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6534 - accuracy: 0.8086 - val_loss: 0.6790 - val_accuracy: 0.8031\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6528 - accuracy: 0.8089 - val_loss: 0.6785 - val_accuracy: 0.8037\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6525 - accuracy: 0.8090 - val_loss: 0.6785 - val_accuracy: 0.8031\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6521 - accuracy: 0.8102 - val_loss: 0.6781 - val_accuracy: 0.8034\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6519 - accuracy: 0.8090 - val_loss: 0.6777 - val_accuracy: 0.8026\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6516 - accuracy: 0.8103 - val_loss: 0.6763 - val_accuracy: 0.8041\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6513 - accuracy: 0.8098 - val_loss: 0.6759 - val_accuracy: 0.8042\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6510 - accuracy: 0.8105 - val_loss: 0.6763 - val_accuracy: 0.8035\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6507 - accuracy: 0.8103 - val_loss: 0.6757 - val_accuracy: 0.8034\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6499 - accuracy: 0.8096 - val_loss: 0.6763 - val_accuracy: 0.8037\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6498 - accuracy: 0.8110 - val_loss: 0.6752 - val_accuracy: 0.8037\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.6494 - accuracy: 0.8105 - val_loss: 0.6747 - val_accuracy: 0.8044\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6488 - accuracy: 0.8107 - val_loss: 0.6778 - val_accuracy: 0.8035\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6489 - accuracy: 0.8105 - val_loss: 0.6747 - val_accuracy: 0.8048\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6485 - accuracy: 0.8104 - val_loss: 0.6761 - val_accuracy: 0.8034\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6482 - accuracy: 0.8108 - val_loss: 0.6740 - val_accuracy: 0.8052\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6481 - accuracy: 0.8109 - val_loss: 0.6728 - val_accuracy: 0.8051\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.6473 - accuracy: 0.8120 - val_loss: 0.6735 - val_accuracy: 0.8044\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6469 - accuracy: 0.8111 - val_loss: 0.6731 - val_accuracy: 0.8049\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6467 - accuracy: 0.8110 - val_loss: 0.6720 - val_accuracy: 0.8051\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.6466 - accuracy: 0.8115 - val_loss: 0.6717 - val_accuracy: 0.8058\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6462 - accuracy: 0.8123 - val_loss: 0.6722 - val_accuracy: 0.8049\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6458 - accuracy: 0.8114 - val_loss: 0.6716 - val_accuracy: 0.8057\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6457 - accuracy: 0.8123 - val_loss: 0.6714 - val_accuracy: 0.8050\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.6452 - accuracy: 0.8117 - val_loss: 0.6718 - val_accuracy: 0.8046\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.6449 - accuracy: 0.8123 - val_loss: 0.6707 - val_accuracy: 0.8061\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6443 - accuracy: 0.8119 - val_loss: 0.6724 - val_accuracy: 0.8054\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6442 - accuracy: 0.8128 - val_loss: 0.6698 - val_accuracy: 0.8060\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and sgd optimizers - changing learning rate'); print('--'*40)\n",
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.001)\n",
    "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.6698 - accuracy: 0.8060\n",
      "Validation accuracy: 80.6\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "NN model with relu activations and adam optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 3.3043 - accuracy: 0.1234 - val_loss: 2.2424 - val_accuracy: 0.1247\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 2.2110 - accuracy: 0.1452 - val_loss: 2.1955 - val_accuracy: 0.1651\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 2.0439 - accuracy: 0.2338 - val_loss: 1.8129 - val_accuracy: 0.3436\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.7344 - accuracy: 0.3807 - val_loss: 1.6330 - val_accuracy: 0.4141\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.5300 - accuracy: 0.4736 - val_loss: 1.4508 - val_accuracy: 0.5065\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 1.3881 - accuracy: 0.5383 - val_loss: 1.2720 - val_accuracy: 0.5837\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 1.3146 - accuracy: 0.5707 - val_loss: 1.2663 - val_accuracy: 0.5897\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.2340 - accuracy: 0.6096 - val_loss: 1.3226 - val_accuracy: 0.5720\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.2137 - accuracy: 0.6187 - val_loss: 1.1703 - val_accuracy: 0.6366\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.1833 - accuracy: 0.6281 - val_loss: 1.2109 - val_accuracy: 0.6150\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 1.1742 - accuracy: 0.6331 - val_loss: 1.1117 - val_accuracy: 0.6549\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 1.1438 - accuracy: 0.6425 - val_loss: 1.0751 - val_accuracy: 0.6666\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.1226 - accuracy: 0.6527 - val_loss: 1.1456 - val_accuracy: 0.6410\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.1068 - accuracy: 0.6589 - val_loss: 1.1636 - val_accuracy: 0.6367\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.1044 - accuracy: 0.6574 - val_loss: 1.1534 - val_accuracy: 0.6373\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0795 - accuracy: 0.6668 - val_loss: 1.0665 - val_accuracy: 0.6714\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 1.0754 - accuracy: 0.6703 - val_loss: 1.0711 - val_accuracy: 0.6707\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0829 - accuracy: 0.6640 - val_loss: 1.1107 - val_accuracy: 0.6539\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0673 - accuracy: 0.6701 - val_loss: 1.1024 - val_accuracy: 0.6636\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0542 - accuracy: 0.6759 - val_loss: 1.0462 - val_accuracy: 0.6775\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0563 - accuracy: 0.6743 - val_loss: 1.1329 - val_accuracy: 0.6391\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0545 - accuracy: 0.6720 - val_loss: 1.1167 - val_accuracy: 0.6487\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 1.0524 - accuracy: 0.6731 - val_loss: 1.0916 - val_accuracy: 0.6674\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0465 - accuracy: 0.6753 - val_loss: 1.0529 - val_accuracy: 0.6755\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0600 - accuracy: 0.6701 - val_loss: 1.0442 - val_accuracy: 0.6768\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0354 - accuracy: 0.6794 - val_loss: 1.0156 - val_accuracy: 0.6888\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0291 - accuracy: 0.6822 - val_loss: 1.0407 - val_accuracy: 0.6739\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 1.0318 - accuracy: 0.6812 - val_loss: 1.0515 - val_accuracy: 0.6716\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0362 - accuracy: 0.6809 - val_loss: 1.0242 - val_accuracy: 0.6870\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0402 - accuracy: 0.6807 - val_loss: 1.0341 - val_accuracy: 0.6815\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0499 - accuracy: 0.6752 - val_loss: 1.0382 - val_accuracy: 0.6804\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0259 - accuracy: 0.6833 - val_loss: 1.0546 - val_accuracy: 0.6761\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 1.0378 - accuracy: 0.6780 - val_loss: 1.0124 - val_accuracy: 0.6893\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0240 - accuracy: 0.6838 - val_loss: 1.0240 - val_accuracy: 0.6846\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0284 - accuracy: 0.6811 - val_loss: 1.0148 - val_accuracy: 0.6866\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0248 - accuracy: 0.6845 - val_loss: 0.9799 - val_accuracy: 0.6998\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0194 - accuracy: 0.6868 - val_loss: 1.0504 - val_accuracy: 0.6769\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 1.0343 - accuracy: 0.6824 - val_loss: 0.9955 - val_accuracy: 0.6962\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0105 - accuracy: 0.6909 - val_loss: 1.0477 - val_accuracy: 0.6747\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0245 - accuracy: 0.6832 - val_loss: 1.0615 - val_accuracy: 0.6641\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0254 - accuracy: 0.6830 - val_loss: 1.0382 - val_accuracy: 0.6778\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0190 - accuracy: 0.6851 - val_loss: 0.9907 - val_accuracy: 0.6960\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0027 - accuracy: 0.6910 - val_loss: 1.0437 - val_accuracy: 0.6767\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0121 - accuracy: 0.6864 - val_loss: 1.0132 - val_accuracy: 0.6866\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0017 - accuracy: 0.6889 - val_loss: 1.0126 - val_accuracy: 0.6870\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0257 - accuracy: 0.6805 - val_loss: 1.0735 - val_accuracy: 0.6665\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9970 - accuracy: 0.6940 - val_loss: 0.9877 - val_accuracy: 0.6981\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9929 - accuracy: 0.6933 - val_loss: 0.9774 - val_accuracy: 0.7001\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0064 - accuracy: 0.6884 - val_loss: 0.9989 - val_accuracy: 0.6946\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 1.0106 - accuracy: 0.6869 - val_loss: 0.9719 - val_accuracy: 0.7047\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 1.0003 - accuracy: 0.6916 - val_loss: 0.9929 - val_accuracy: 0.6959\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9956 - accuracy: 0.6935 - val_loss: 1.0854 - val_accuracy: 0.6639\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 1.0013 - accuracy: 0.6895 - val_loss: 1.0078 - val_accuracy: 0.6952\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.9885 - accuracy: 0.6968 - val_loss: 0.9696 - val_accuracy: 0.7021\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9823 - accuracy: 0.6980 - val_loss: 1.0683 - val_accuracy: 0.6715\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9941 - accuracy: 0.6928 - val_loss: 0.9782 - val_accuracy: 0.6983\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9812 - accuracy: 0.6975 - val_loss: 1.0006 - val_accuracy: 0.6937\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9875 - accuracy: 0.6967 - val_loss: 1.0053 - val_accuracy: 0.6878\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9909 - accuracy: 0.6964 - val_loss: 0.9611 - val_accuracy: 0.7053\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9824 - accuracy: 0.6968 - val_loss: 1.0101 - val_accuracy: 0.6913\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9832 - accuracy: 0.6983 - val_loss: 0.9679 - val_accuracy: 0.7042\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.9753 - accuracy: 0.7000 - val_loss: 0.9950 - val_accuracy: 0.6910\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9871 - accuracy: 0.6940 - val_loss: 0.9920 - val_accuracy: 0.6936\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9696 - accuracy: 0.7033 - val_loss: 0.9920 - val_accuracy: 0.6965\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9871 - accuracy: 0.6963 - val_loss: 1.0198 - val_accuracy: 0.6845\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9644 - accuracy: 0.7042 - val_loss: 0.9325 - val_accuracy: 0.7175\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9575 - accuracy: 0.7064 - val_loss: 0.9670 - val_accuracy: 0.7045\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9797 - accuracy: 0.7008 - val_loss: 0.9673 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9570 - accuracy: 0.7058 - val_loss: 0.9731 - val_accuracy: 0.6984\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9675 - accuracy: 0.7023 - val_loss: 0.9272 - val_accuracy: 0.7150\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9727 - accuracy: 0.7024 - val_loss: 0.9444 - val_accuracy: 0.7102\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9597 - accuracy: 0.7065 - val_loss: 1.0060 - val_accuracy: 0.6922\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9684 - accuracy: 0.7007 - val_loss: 0.9901 - val_accuracy: 0.6921\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9579 - accuracy: 0.7046 - val_loss: 0.9949 - val_accuracy: 0.6878\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.9657 - accuracy: 0.7007 - val_loss: 1.0542 - val_accuracy: 0.6776\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9665 - accuracy: 0.7034 - val_loss: 0.9415 - val_accuracy: 0.7099\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9590 - accuracy: 0.7048 - val_loss: 0.9605 - val_accuracy: 0.7055\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9571 - accuracy: 0.7050 - val_loss: 0.9845 - val_accuracy: 0.6983\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9655 - accuracy: 0.7039 - val_loss: 0.9464 - val_accuracy: 0.7094\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9591 - accuracy: 0.7043 - val_loss: 0.9387 - val_accuracy: 0.7112\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9507 - accuracy: 0.7088 - val_loss: 0.9815 - val_accuracy: 0.6984\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9544 - accuracy: 0.7058 - val_loss: 0.9452 - val_accuracy: 0.7141\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9488 - accuracy: 0.7067 - val_loss: 0.9517 - val_accuracy: 0.7084\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9596 - accuracy: 0.7055 - val_loss: 0.9581 - val_accuracy: 0.7072\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9651 - accuracy: 0.7040 - val_loss: 0.9618 - val_accuracy: 0.7060\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9482 - accuracy: 0.7083 - val_loss: 0.9785 - val_accuracy: 0.7015\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9410 - accuracy: 0.7120 - val_loss: 0.9367 - val_accuracy: 0.7134\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9495 - accuracy: 0.7085 - val_loss: 0.9258 - val_accuracy: 0.7212\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9456 - accuracy: 0.7090 - val_loss: 0.9700 - val_accuracy: 0.7009\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9471 - accuracy: 0.7090 - val_loss: 0.9416 - val_accuracy: 0.7099\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9468 - accuracy: 0.7088 - val_loss: 0.9941 - val_accuracy: 0.6926\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9475 - accuracy: 0.7092 - val_loss: 0.9210 - val_accuracy: 0.7192\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9428 - accuracy: 0.7093 - val_loss: 0.9425 - val_accuracy: 0.7137\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.9405 - accuracy: 0.7127 - val_loss: 0.9107 - val_accuracy: 0.7230\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9378 - accuracy: 0.7128 - val_loss: 0.9263 - val_accuracy: 0.7178\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9243 - accuracy: 0.7152 - val_loss: 0.9747 - val_accuracy: 0.7016\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9402 - accuracy: 0.7117 - val_loss: 1.0209 - val_accuracy: 0.6854\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9334 - accuracy: 0.7140 - val_loss: 0.9202 - val_accuracy: 0.7211\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9371 - accuracy: 0.7123 - val_loss: 0.9505 - val_accuracy: 0.7086\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9286 - accuracy: 0.7158 - val_loss: 0.9943 - val_accuracy: 0.6916\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.01)\n",
    "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.9943 - accuracy: 0.6916\n",
      "Validation accuracy: 69.16\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, adam optimizer, changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "NN model with relu activations and adam optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.8512 - accuracy: 0.7421 - val_loss: 0.8583 - val_accuracy: 0.7405\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.8413 - accuracy: 0.7432 - val_loss: 0.8521 - val_accuracy: 0.7419\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.8396 - accuracy: 0.7439 - val_loss: 0.8550 - val_accuracy: 0.7398\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8376 - accuracy: 0.7460 - val_loss: 0.8507 - val_accuracy: 0.7420\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8363 - accuracy: 0.7450 - val_loss: 0.8587 - val_accuracy: 0.7386\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8363 - accuracy: 0.7446 - val_loss: 0.8582 - val_accuracy: 0.7394\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8343 - accuracy: 0.7453 - val_loss: 0.8508 - val_accuracy: 0.7424\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 0.8354 - accuracy: 0.7463 - val_loss: 0.8586 - val_accuracy: 0.7379\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8345 - accuracy: 0.7464 - val_loss: 0.8458 - val_accuracy: 0.7442\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8341 - accuracy: 0.7456 - val_loss: 0.8589 - val_accuracy: 0.7383\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8325 - accuracy: 0.7451 - val_loss: 0.8490 - val_accuracy: 0.7424\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8345 - accuracy: 0.7464 - val_loss: 0.8534 - val_accuracy: 0.7416\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8324 - accuracy: 0.7462 - val_loss: 0.8506 - val_accuracy: 0.7423\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8326 - accuracy: 0.7455 - val_loss: 0.8508 - val_accuracy: 0.7420\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8333 - accuracy: 0.7460 - val_loss: 0.8468 - val_accuracy: 0.7449\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8288 - accuracy: 0.7470 - val_loss: 0.8531 - val_accuracy: 0.7399\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8307 - accuracy: 0.7457 - val_loss: 0.8522 - val_accuracy: 0.7419\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8321 - accuracy: 0.7445 - val_loss: 0.8455 - val_accuracy: 0.7432\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8278 - accuracy: 0.7489 - val_loss: 0.8455 - val_accuracy: 0.7438\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8280 - accuracy: 0.7472 - val_loss: 0.8585 - val_accuracy: 0.7398\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8293 - accuracy: 0.7467 - val_loss: 0.8436 - val_accuracy: 0.7443\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8295 - accuracy: 0.7471 - val_loss: 0.8430 - val_accuracy: 0.7448\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8267 - accuracy: 0.7482 - val_loss: 0.8541 - val_accuracy: 0.7410\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8286 - accuracy: 0.7478 - val_loss: 0.8453 - val_accuracy: 0.7431\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8255 - accuracy: 0.7479 - val_loss: 0.8422 - val_accuracy: 0.7452\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8270 - accuracy: 0.7479 - val_loss: 0.8450 - val_accuracy: 0.7439\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8274 - accuracy: 0.7480 - val_loss: 0.8494 - val_accuracy: 0.7426\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8263 - accuracy: 0.7475 - val_loss: 0.8451 - val_accuracy: 0.7448\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.8258 - accuracy: 0.7479 - val_loss: 0.8454 - val_accuracy: 0.7445\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8265 - accuracy: 0.7486 - val_loss: 0.8498 - val_accuracy: 0.7422\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8289 - accuracy: 0.7477 - val_loss: 0.8465 - val_accuracy: 0.7437\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8257 - accuracy: 0.7488 - val_loss: 0.8445 - val_accuracy: 0.7444\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8248 - accuracy: 0.7505 - val_loss: 0.8438 - val_accuracy: 0.7437\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.8247 - accuracy: 0.7470 - val_loss: 0.8405 - val_accuracy: 0.7463\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8224 - accuracy: 0.7490 - val_loss: 0.8409 - val_accuracy: 0.7458\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8241 - accuracy: 0.7492 - val_loss: 0.8404 - val_accuracy: 0.7449\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8261 - accuracy: 0.7466 - val_loss: 0.8506 - val_accuracy: 0.7415\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8244 - accuracy: 0.7486 - val_loss: 0.8439 - val_accuracy: 0.7442\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.8234 - accuracy: 0.7489 - val_loss: 0.8452 - val_accuracy: 0.7439\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8241 - accuracy: 0.7489 - val_loss: 0.8417 - val_accuracy: 0.7449\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8247 - accuracy: 0.7479 - val_loss: 0.8417 - val_accuracy: 0.7457\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8235 - accuracy: 0.7489 - val_loss: 0.8485 - val_accuracy: 0.7428\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8231 - accuracy: 0.7477 - val_loss: 0.8429 - val_accuracy: 0.7441\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8235 - accuracy: 0.7490 - val_loss: 0.8535 - val_accuracy: 0.7394\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8243 - accuracy: 0.7472 - val_loss: 0.8377 - val_accuracy: 0.7467\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8217 - accuracy: 0.7490 - val_loss: 0.8388 - val_accuracy: 0.7459\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8238 - accuracy: 0.7484 - val_loss: 0.8508 - val_accuracy: 0.7411\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8236 - accuracy: 0.7487 - val_loss: 0.8437 - val_accuracy: 0.7438\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8216 - accuracy: 0.7479 - val_loss: 0.8390 - val_accuracy: 0.7466\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 3s 76us/sample - loss: 0.8192 - accuracy: 0.7493 - val_loss: 0.8446 - val_accuracy: 0.7452\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8215 - accuracy: 0.7489 - val_loss: 0.8411 - val_accuracy: 0.7452\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8221 - accuracy: 0.7487 - val_loss: 0.8485 - val_accuracy: 0.7407\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8201 - accuracy: 0.7501 - val_loss: 0.8380 - val_accuracy: 0.7464\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8218 - accuracy: 0.7483 - val_loss: 0.8435 - val_accuracy: 0.7446\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8219 - accuracy: 0.7482 - val_loss: 0.8425 - val_accuracy: 0.7439\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8191 - accuracy: 0.7490 - val_loss: 0.8435 - val_accuracy: 0.7446\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8197 - accuracy: 0.7489 - val_loss: 0.8402 - val_accuracy: 0.7454\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8219 - accuracy: 0.7465 - val_loss: 0.8400 - val_accuracy: 0.7452\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8202 - accuracy: 0.7491 - val_loss: 0.8437 - val_accuracy: 0.7446\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.8212 - accuracy: 0.7496 - val_loss: 0.8434 - val_accuracy: 0.7442\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8188 - accuracy: 0.7502 - val_loss: 0.8398 - val_accuracy: 0.7456\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8208 - accuracy: 0.7490 - val_loss: 0.8371 - val_accuracy: 0.7476\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8194 - accuracy: 0.7503 - val_loss: 0.8389 - val_accuracy: 0.7473\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8201 - accuracy: 0.7489 - val_loss: 0.8482 - val_accuracy: 0.7417\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8185 - accuracy: 0.7494 - val_loss: 0.8510 - val_accuracy: 0.7408\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8187 - accuracy: 0.7506 - val_loss: 0.8441 - val_accuracy: 0.7445\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8191 - accuracy: 0.7503 - val_loss: 0.8395 - val_accuracy: 0.7454\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8216 - accuracy: 0.7490 - val_loss: 0.8335 - val_accuracy: 0.7479\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8196 - accuracy: 0.7507 - val_loss: 0.8353 - val_accuracy: 0.7483\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8169 - accuracy: 0.7500 - val_loss: 0.8381 - val_accuracy: 0.7456\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8161 - accuracy: 0.7506 - val_loss: 0.8358 - val_accuracy: 0.7460\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8179 - accuracy: 0.7495 - val_loss: 0.8435 - val_accuracy: 0.7445\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8167 - accuracy: 0.7506 - val_loss: 0.8415 - val_accuracy: 0.7448ccuracy: \n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8187 - accuracy: 0.7499 - val_loss: 0.8395 - val_accuracy: 0.7452\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8173 - accuracy: 0.7503 - val_loss: 0.8349 - val_accuracy: 0.7473\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8172 - accuracy: 0.7507 - val_loss: 0.8369 - val_accuracy: 0.7459\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8174 - accuracy: 0.7505 - val_loss: 0.8440 - val_accuracy: 0.7437\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8183 - accuracy: 0.7509 - val_loss: 0.8422 - val_accuracy: 0.7433\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8181 - accuracy: 0.7496 - val_loss: 0.8359 - val_accuracy: 0.7468\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8151 - accuracy: 0.7510 - val_loss: 0.8399 - val_accuracy: 0.7441\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.8184 - accuracy: 0.7496 - val_loss: 0.8372 - val_accuracy: 0.7472\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8166 - accuracy: 0.7501 - val_loss: 0.8356 - val_accuracy: 0.7469\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.8179 - accuracy: 0.7496 - val_loss: 0.8427 - val_accuracy: 0.7452\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 0.8173 - accuracy: 0.7507 - val_loss: 0.8322 - val_accuracy: 0.7489\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8166 - accuracy: 0.7503 - val_loss: 0.8405 - val_accuracy: 0.7450\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8159 - accuracy: 0.7499 - val_loss: 0.8431 - val_accuracy: 0.7436\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.8162 - accuracy: 0.7500 - val_loss: 0.8412 - val_accuracy: 0.7449\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.8168 - accuracy: 0.7505 - val_loss: 0.8407 - val_accuracy: 0.7447\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.8157 - accuracy: 0.7506 - val_loss: 0.8358 - val_accuracy: 0.7474\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8151 - accuracy: 0.7510 - val_loss: 0.8351 - val_accuracy: 0.7463\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.8151 - accuracy: 0.7505 - val_loss: 0.8364 - val_accuracy: 0.7474\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8145 - accuracy: 0.7516 - val_loss: 0.8380 - val_accuracy: 0.7473\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8139 - accuracy: 0.7510 - val_loss: 0.8356 - val_accuracy: 0.7462\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8168 - accuracy: 0.7506 - val_loss: 0.8466 - val_accuracy: 0.7429\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 3s 77us/sample - loss: 0.8157 - accuracy: 0.7508 - val_loss: 0.8419 - val_accuracy: 0.7451\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.8153 - accuracy: 0.7520 - val_loss: 0.8338 - val_accuracy: 0.7474\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8151 - accuracy: 0.7528 - val_loss: 0.8323 - val_accuracy: 0.7473\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8152 - accuracy: 0.7508 - val_loss: 0.8366 - val_accuracy: 0.7473\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8136 - accuracy: 0.7512 - val_loss: 0.8382 - val_accuracy: 0.7464\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8144 - accuracy: 0.7509 - val_loss: 0.8347 - val_accuracy: 0.7471\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.8347 - accuracy: 0.7471\n",
      "Validation accuracy: 74.71\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation - NN model with relu activations\n",
    "Improves the scores considerably.\n",
    "Best accuracy achieved using relu activations, SGD optimizer, changing learning rate to 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, changing number of activators, SGD optimizers after changing number of activators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with relu activations and changing number of activators\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model3 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu\n",
    "model3.add(Dense(256, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model3.add(Dense(128))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - Adding second hidden layer\n",
    "model3.add(Dense(64))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model3.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 304,202\n",
      "Trainable params: 304,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 2.2975 - accuracy: 0.1270 - val_loss: 2.2879 - val_accuracy: 0.1415\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 2.2806 - accuracy: 0.1650 - val_loss: 2.2721 - val_accuracy: 0.1831\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 2.2642 - accuracy: 0.2020 - val_loss: 2.2553 - val_accuracy: 0.2164\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 2.2440 - accuracy: 0.2331 - val_loss: 2.2312 - val_accuracy: 0.2519\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 2.2180 - accuracy: 0.2603 - val_loss: 2.2002 - val_accuracy: 0.2794\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 2.1825 - accuracy: 0.2839 - val_loss: 2.1600 - val_accuracy: 0.2971\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 2.1352 - accuracy: 0.3156 - val_loss: 2.1049 - val_accuracy: 0.3452\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 2.0736 - accuracy: 0.3548 - val_loss: 2.0338 - val_accuracy: 0.3871\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 1.9959 - accuracy: 0.3877 - val_loss: 1.9482 - val_accuracy: 0.4105\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 1.9065 - accuracy: 0.4197 - val_loss: 1.8657 - val_accuracy: 0.4275\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 1.8104 - accuracy: 0.4498 - val_loss: 1.7616 - val_accuracy: 0.4516\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 1.7162 - accuracy: 0.4789 - val_loss: 1.6745 - val_accuracy: 0.4861\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.6307 - accuracy: 0.5060 - val_loss: 1.5837 - val_accuracy: 0.5146\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 1.5573 - accuracy: 0.5277 - val_loss: 1.5336 - val_accuracy: 0.5225\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 1.4935 - accuracy: 0.5462 - val_loss: 1.4868 - val_accuracy: 0.5367\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.4390 - accuracy: 0.5616 - val_loss: 1.4890 - val_accuracy: 0.5213\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.3860 - accuracy: 0.5767 - val_loss: 1.3557 - val_accuracy: 0.5876\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 1.3511 - accuracy: 0.5855 - val_loss: 1.3025 - val_accuracy: 0.6125\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 1.3163 - accuracy: 0.5978 - val_loss: 1.2643 - val_accuracy: 0.6213\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 1.2793 - accuracy: 0.6074 - val_loss: 1.2634 - val_accuracy: 0.6168\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 1.2525 - accuracy: 0.6157 - val_loss: 1.2073 - val_accuracy: 0.6401\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 1.2228 - accuracy: 0.6249 - val_loss: 1.1923 - val_accuracy: 0.6396\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.1962 - accuracy: 0.6342 - val_loss: 1.1910 - val_accuracy: 0.6348\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 1.1729 - accuracy: 0.6425 - val_loss: 1.1490 - val_accuracy: 0.6505\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 1.1507 - accuracy: 0.6487 - val_loss: 1.1435 - val_accuracy: 0.6529\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 1.1313 - accuracy: 0.6556 - val_loss: 1.1309 - val_accuracy: 0.6527\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.1134 - accuracy: 0.6606 - val_loss: 1.0954 - val_accuracy: 0.6691\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 1.0992 - accuracy: 0.6638 - val_loss: 1.0946 - val_accuracy: 0.6683\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 1.0817 - accuracy: 0.6698 - val_loss: 1.0599 - val_accuracy: 0.6806\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 1.0675 - accuracy: 0.6754 - val_loss: 1.0481 - val_accuracy: 0.6815\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 1.0551 - accuracy: 0.6778 - val_loss: 1.0915 - val_accuracy: 0.6582\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 1.0366 - accuracy: 0.6857 - val_loss: 1.0499 - val_accuracy: 0.6781\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 1.0258 - accuracy: 0.6888 - val_loss: 1.0003 - val_accuracy: 0.7006\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 1.0112 - accuracy: 0.6944 - val_loss: 1.0556 - val_accuracy: 0.6781\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 1.0033 - accuracy: 0.6960 - val_loss: 0.9823 - val_accuracy: 0.7037\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.9924 - accuracy: 0.7004 - val_loss: 0.9722 - val_accuracy: 0.7078\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.9783 - accuracy: 0.7025 - val_loss: 0.9622 - val_accuracy: 0.7140\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.9657 - accuracy: 0.7072 - val_loss: 0.9561 - val_accuracy: 0.7148\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.9621 - accuracy: 0.7091 - val_loss: 0.9474 - val_accuracy: 0.7148\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.9510 - accuracy: 0.7128 - val_loss: 0.9440 - val_accuracy: 0.7155\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.9364 - accuracy: 0.7161 - val_loss: 0.9194 - val_accuracy: 0.7234\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.9310 - accuracy: 0.7188 - val_loss: 0.9208 - val_accuracy: 0.7222\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.9230 - accuracy: 0.7190 - val_loss: 0.9086 - val_accuracy: 0.7262\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.9134 - accuracy: 0.7242 - val_loss: 0.9146 - val_accuracy: 0.7213\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.9058 - accuracy: 0.7255 - val_loss: 0.8946 - val_accuracy: 0.7310\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.8960 - accuracy: 0.7303 - val_loss: 0.9260 - val_accuracy: 0.7152\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.8907 - accuracy: 0.7295 - val_loss: 0.8736 - val_accuracy: 0.7380\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.8813 - accuracy: 0.7322 - val_loss: 0.8923 - val_accuracy: 0.7262\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.8767 - accuracy: 0.7344 - val_loss: 0.8711 - val_accuracy: 0.7358\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.8671 - accuracy: 0.7354 - val_loss: 0.8696 - val_accuracy: 0.7396\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.8614 - accuracy: 0.7406 - val_loss: 0.8690 - val_accuracy: 0.7362\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.8524 - accuracy: 0.7416 - val_loss: 0.8552 - val_accuracy: 0.7407\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.8478 - accuracy: 0.7421 - val_loss: 0.8380 - val_accuracy: 0.7500\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.8386 - accuracy: 0.7464 - val_loss: 0.8282 - val_accuracy: 0.7518\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.8302 - accuracy: 0.7485 - val_loss: 0.8303 - val_accuracy: 0.7505\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.8262 - accuracy: 0.7488 - val_loss: 0.8279 - val_accuracy: 0.7498\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.8205 - accuracy: 0.7514 - val_loss: 0.8359 - val_accuracy: 0.7483\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.8146 - accuracy: 0.7536 - val_loss: 0.8102 - val_accuracy: 0.7562\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.8064 - accuracy: 0.7557 - val_loss: 0.8260 - val_accuracy: 0.7484\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.8040 - accuracy: 0.7566 - val_loss: 0.7980 - val_accuracy: 0.7600\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.7938 - accuracy: 0.7594 - val_loss: 0.7958 - val_accuracy: 0.7612\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.7873 - accuracy: 0.7639 - val_loss: 0.7993 - val_accuracy: 0.7576\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.7867 - accuracy: 0.7626 - val_loss: 0.7854 - val_accuracy: 0.7641\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.7848 - accuracy: 0.7603 - val_loss: 0.7776 - val_accuracy: 0.7657\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.7753 - accuracy: 0.7656 - val_loss: 0.7868 - val_accuracy: 0.7649\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.7710 - accuracy: 0.7652 - val_loss: 0.7736 - val_accuracy: 0.7669\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.7647 - accuracy: 0.7685 - val_loss: 0.7923 - val_accuracy: 0.7586\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.7587 - accuracy: 0.7715 - val_loss: 0.7687 - val_accuracy: 0.7672\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.7554 - accuracy: 0.7705 - val_loss: 0.7815 - val_accuracy: 0.7645\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.7499 - accuracy: 0.7741 - val_loss: 0.7562 - val_accuracy: 0.7711\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.7447 - accuracy: 0.7742 - val_loss: 0.7538 - val_accuracy: 0.7727\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.7355 - accuracy: 0.7787 - val_loss: 0.7402 - val_accuracy: 0.7799\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.7388 - accuracy: 0.7757 - val_loss: 0.7379 - val_accuracy: 0.7787\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.7334 - accuracy: 0.7762 - val_loss: 0.7676 - val_accuracy: 0.7688- accuracy: 0.\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.7267 - accuracy: 0.7795 - val_loss: 0.7343 - val_accuracy: 0.7775\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.7217 - accuracy: 0.7816 - val_loss: 0.7259 - val_accuracy: 0.7825\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.7190 - accuracy: 0.7833 - val_loss: 0.7328 - val_accuracy: 0.7801\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.7129 - accuracy: 0.7851 - val_loss: 0.7238 - val_accuracy: 0.7825\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.7121 - accuracy: 0.7856 - val_loss: 0.7218 - val_accuracy: 0.7827\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.7042 - accuracy: 0.7895 - val_loss: 0.7216 - val_accuracy: 0.7812\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.7039 - accuracy: 0.7864 - val_loss: 0.7227 - val_accuracy: 0.7827\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.6986 - accuracy: 0.7902 - val_loss: 0.6995 - val_accuracy: 0.7905\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6916 - accuracy: 0.7922 - val_loss: 0.6950 - val_accuracy: 0.7924\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 5s 118us/sample - loss: 0.6888 - accuracy: 0.7918 - val_loss: 0.7278 - val_accuracy: 0.7789\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6874 - accuracy: 0.7932 - val_loss: 0.6901 - val_accuracy: 0.7946\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.6816 - accuracy: 0.7938 - val_loss: 0.7224 - val_accuracy: 0.7811\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.6764 - accuracy: 0.7968 - val_loss: 0.7050 - val_accuracy: 0.7855\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.6725 - accuracy: 0.7975 - val_loss: 0.6824 - val_accuracy: 0.7983\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.6684 - accuracy: 0.7985 - val_loss: 0.7019 - val_accuracy: 0.7869\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.6640 - accuracy: 0.7990 - val_loss: 0.6933 - val_accuracy: 0.7913\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.6627 - accuracy: 0.7989 - val_loss: 0.6879 - val_accuracy: 0.7939\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.6581 - accuracy: 0.8015 - val_loss: 0.6767 - val_accuracy: 0.7976\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.6526 - accuracy: 0.8032 - val_loss: 0.6522 - val_accuracy: 0.8065\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.6495 - accuracy: 0.8018 - val_loss: 0.6531 - val_accuracy: 0.8058\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6469 - accuracy: 0.8049 - val_loss: 0.6772 - val_accuracy: 0.7958\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6467 - accuracy: 0.8048 - val_loss: 0.6687 - val_accuracy: 0.7997\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.6386 - accuracy: 0.8087 - val_loss: 0.6577 - val_accuracy: 0.8033\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.6355 - accuracy: 0.8090 - val_loss: 0.6651 - val_accuracy: 0.8034\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6307 - accuracy: 0.8108 - val_loss: 0.6513 - val_accuracy: 0.8055\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.6323 - accuracy: 0.8096 - val_loss: 0.6613 - val_accuracy: 0.7992\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model3.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations and changing the number of activators\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.6613 - accuracy: 0.7992\n",
      "Validation accuracy: 79.92\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
    "results3 = model3.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, changing number of activators, Adam optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.9187 - accuracy: 0.7191 - val_loss: 0.9018 - val_accuracy: 0.7165\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.8046 - accuracy: 0.7522 - val_loss: 0.8075 - val_accuracy: 0.7555\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.7712 - accuracy: 0.7633 - val_loss: 0.7746 - val_accuracy: 0.7637\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 5s 118us/sample - loss: 0.7546 - accuracy: 0.7689 - val_loss: 0.7140 - val_accuracy: 0.7838\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.7401 - accuracy: 0.7745 - val_loss: 0.7923 - val_accuracy: 0.7532\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.7102 - accuracy: 0.7823 - val_loss: 0.7045 - val_accuracy: 0.7819\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.6939 - accuracy: 0.7861 - val_loss: 0.7288 - val_accuracy: 0.7784\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.6924 - accuracy: 0.7863 - val_loss: 0.6757 - val_accuracy: 0.7968\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6748 - accuracy: 0.7923 - val_loss: 0.6376 - val_accuracy: 0.8079\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.6353 - accuracy: 0.8045 - val_loss: 0.6245 - val_accuracy: 0.8123\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.6275 - accuracy: 0.8096 - val_loss: 0.6521 - val_accuracy: 0.8003\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.6080 - accuracy: 0.8160 - val_loss: 0.6172 - val_accuracy: 0.8173\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.6062 - accuracy: 0.8146 - val_loss: 0.5968 - val_accuracy: 0.8208\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.5963 - accuracy: 0.8175 - val_loss: 0.6245 - val_accuracy: 0.8092\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.5875 - accuracy: 0.8205 - val_loss: 0.6386 - val_accuracy: 0.8040\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.5787 - accuracy: 0.8236 - val_loss: 0.5606 - val_accuracy: 0.8329\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.5541 - accuracy: 0.8290 - val_loss: 0.5784 - val_accuracy: 0.8243\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.5528 - accuracy: 0.8297 - val_loss: 0.6181 - val_accuracy: 0.8099\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.5566 - accuracy: 0.8287 - val_loss: 0.5737 - val_accuracy: 0.8259\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.5437 - accuracy: 0.8316 - val_loss: 0.5706 - val_accuracy: 0.8261\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.5408 - accuracy: 0.8320 - val_loss: 0.5660 - val_accuracy: 0.8274\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.5375 - accuracy: 0.8332 - val_loss: 0.5654 - val_accuracy: 0.8299\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.5160 - accuracy: 0.8395 - val_loss: 0.5506 - val_accuracy: 0.8368\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.5018 - accuracy: 0.8454 - val_loss: 0.5026 - val_accuracy: 0.8484\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.4948 - accuracy: 0.8461 - val_loss: 0.5267 - val_accuracy: 0.8396\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.4927 - accuracy: 0.8484 - val_loss: 0.5344 - val_accuracy: 0.8360\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.4735 - accuracy: 0.8524 - val_loss: 0.4883 - val_accuracy: 0.8536\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.4738 - accuracy: 0.8518 - val_loss: 0.5094 - val_accuracy: 0.8457\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.4769 - accuracy: 0.8507 - val_loss: 0.6196 - val_accuracy: 0.8102\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.4708 - accuracy: 0.8539 - val_loss: 0.5137 - val_accuracy: 0.8441\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.4678 - accuracy: 0.8547 - val_loss: 0.4894 - val_accuracy: 0.8529\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.4553 - accuracy: 0.8597 - val_loss: 0.5125 - val_accuracy: 0.8458\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.4526 - accuracy: 0.8586 - val_loss: 0.5231 - val_accuracy: 0.8408\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.4475 - accuracy: 0.8599 - val_loss: 0.5237 - val_accuracy: 0.8439\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.4518 - accuracy: 0.8594 - val_loss: 0.5106 - val_accuracy: 0.8450\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.4328 - accuracy: 0.8648 - val_loss: 0.5173 - val_accuracy: 0.8448\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.4335 - accuracy: 0.8646 - val_loss: 0.4682 - val_accuracy: 0.8607\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 5s 118us/sample - loss: 0.4303 - accuracy: 0.8655 - val_loss: 0.5430 - val_accuracy: 0.8373\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.4336 - accuracy: 0.8642 - val_loss: 0.4546 - val_accuracy: 0.8632\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.4148 - accuracy: 0.8699 - val_loss: 0.5429 - val_accuracy: 0.8407\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 5s 119us/sample - loss: 0.4051 - accuracy: 0.8737 - val_loss: 0.4438 - val_accuracy: 0.8700\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.4090 - accuracy: 0.8721 - val_loss: 0.4357 - val_accuracy: 0.8715\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3929 - accuracy: 0.8772 - val_loss: 0.4465 - val_accuracy: 0.8685\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.3946 - accuracy: 0.8773 - val_loss: 0.5071 - val_accuracy: 0.8481\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.3949 - accuracy: 0.8761 - val_loss: 0.4373 - val_accuracy: 0.8699\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3971 - accuracy: 0.8752 - val_loss: 0.4651 - val_accuracy: 0.8630\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.3803 - accuracy: 0.8791 - val_loss: 0.4512 - val_accuracy: 0.8655\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 5s 120us/sample - loss: 0.3812 - accuracy: 0.8792 - val_loss: 0.4545 - val_accuracy: 0.8648\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.3820 - accuracy: 0.8796 - val_loss: 0.4298 - val_accuracy: 0.8731\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.3745 - accuracy: 0.8808 - val_loss: 0.4428 - val_accuracy: 0.8695\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 5s 118us/sample - loss: 0.3634 - accuracy: 0.8853 - val_loss: 0.4701 - val_accuracy: 0.8591\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3786 - accuracy: 0.8809 - val_loss: 0.4061 - val_accuracy: 0.8803\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3641 - accuracy: 0.8841 - val_loss: 0.4365 - val_accuracy: 0.8683\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.3487 - accuracy: 0.8895 - val_loss: 0.4206 - val_accuracy: 0.8760\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.3599 - accuracy: 0.8862 - val_loss: 0.4127 - val_accuracy: 0.8794\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.3600 - accuracy: 0.8845 - val_loss: 0.4358 - val_accuracy: 0.8715\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.3509 - accuracy: 0.8904 - val_loss: 0.3940 - val_accuracy: 0.8844\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.3473 - accuracy: 0.8899 - val_loss: 0.4043 - val_accuracy: 0.8816\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.3421 - accuracy: 0.8921 - val_loss: 0.4221 - val_accuracy: 0.8751\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3408 - accuracy: 0.8913 - val_loss: 0.4275 - val_accuracy: 0.8753\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.3363 - accuracy: 0.8927 - val_loss: 0.3947 - val_accuracy: 0.8849\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.3242 - accuracy: 0.8966 - val_loss: 0.4070 - val_accuracy: 0.8817\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3262 - accuracy: 0.8952 - val_loss: 0.4189 - val_accuracy: 0.8763\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.3323 - accuracy: 0.8937 - val_loss: 0.4359 - val_accuracy: 0.8711\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 5s 114us/sample - loss: 0.3204 - accuracy: 0.8973 - val_loss: 0.3855 - val_accuracy: 0.8871\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.3244 - accuracy: 0.8958 - val_loss: 0.4093 - val_accuracy: 0.8814\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.3227 - accuracy: 0.8970 - val_loss: 0.4350 - val_accuracy: 0.8741\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.3131 - accuracy: 0.8997 - val_loss: 0.4214 - val_accuracy: 0.8763\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.3115 - accuracy: 0.8997 - val_loss: 0.3909 - val_accuracy: 0.8883\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.3152 - accuracy: 0.8983 - val_loss: 0.3916 - val_accuracy: 0.8887\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.3169 - accuracy: 0.8981 - val_loss: 0.4773 - val_accuracy: 0.8596\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.2943 - accuracy: 0.9054 - val_loss: 0.4300 - val_accuracy: 0.8732\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.3124 - accuracy: 0.8997 - val_loss: 0.4073 - val_accuracy: 0.8836\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.3089 - accuracy: 0.9002 - val_loss: 0.4038 - val_accuracy: 0.8839\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.2959 - accuracy: 0.9056 - val_loss: 0.3801 - val_accuracy: 0.8909\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.2886 - accuracy: 0.9065 - val_loss: 0.3738 - val_accuracy: 0.8937\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.2868 - accuracy: 0.9075 - val_loss: 0.4221 - val_accuracy: 0.8809\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.3079 - accuracy: 0.9005 - val_loss: 0.3730 - val_accuracy: 0.8945\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.2972 - accuracy: 0.9041 - val_loss: 0.4394 - val_accuracy: 0.8747\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.2829 - accuracy: 0.9086 - val_loss: 0.3993 - val_accuracy: 0.8876\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 5s 119us/sample - loss: 0.2902 - accuracy: 0.9052 - val_loss: 0.4018 - val_accuracy: 0.8866\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 5s 120us/sample - loss: 0.2767 - accuracy: 0.9116 - val_loss: 0.3643 - val_accuracy: 0.8993\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.2714 - accuracy: 0.9116 - val_loss: 0.4048 - val_accuracy: 0.8865\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.2773 - accuracy: 0.9102 - val_loss: 0.3758 - val_accuracy: 0.8950\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.2612 - accuracy: 0.9159 - val_loss: 0.4037 - val_accuracy: 0.8860\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 5s 110us/sample - loss: 0.2621 - accuracy: 0.9159 - val_loss: 0.3954 - val_accuracy: 0.8911\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.2738 - accuracy: 0.9108 - val_loss: 0.3812 - val_accuracy: 0.8931\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.2724 - accuracy: 0.9115 - val_loss: 0.4414 - val_accuracy: 0.8781\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.2790 - accuracy: 0.9105 - val_loss: 0.3979 - val_accuracy: 0.8870\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.2503 - accuracy: 0.9190 - val_loss: 0.3744 - val_accuracy: 0.8972\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.2600 - accuracy: 0.9163 - val_loss: 0.4094 - val_accuracy: 0.8862\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 5s 119us/sample - loss: 0.2607 - accuracy: 0.9147 - val_loss: 0.3708 - val_accuracy: 0.8996\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.2529 - accuracy: 0.9183 - val_loss: 0.3725 - val_accuracy: 0.8989\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.2450 - accuracy: 0.9194 - val_loss: 0.3761 - val_accuracy: 0.8990\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.2517 - accuracy: 0.9179 - val_loss: 0.3670 - val_accuracy: 0.9004\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.2634 - accuracy: 0.9140 - val_loss: 0.3973 - val_accuracy: 0.8906\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.2479 - accuracy: 0.9183 - val_loss: 0.3555 - val_accuracy: 0.9063\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 5s 116us/sample - loss: 0.2555 - accuracy: 0.9155 - val_loss: 0.3556 - val_accuracy: 0.9053\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 0.2300 - accuracy: 0.9253 - val_loss: 0.3712 - val_accuracy: 0.8978\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 5s 113us/sample - loss: 0.2483 - accuracy: 0.9197 - val_loss: 0.3664 - val_accuracy: 0.9014\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations and changing the number of activators\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.3664 - accuracy: 0.9014\n",
      "Validation accuracy: 90.14\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
    "results3 = model3.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation - NN model with relu activations and changing activators\n",
    "Adding relu activations and changing activators results in improvement of score.\n",
    "Best accuracy achieved using relu activations, changing number of activators and Adam optimizers with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, SGD optimizers with weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with weight initializers\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with weight initializers'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model4 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
    "model4.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model4.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - adding second hidden layer\n",
    "model4.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 3 - adding third hidden layer\n",
    "model4.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model4.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 305,962\n",
      "Trainable params: 305,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 2.3061 - accuracy: 0.1179 - val_loss: 2.2759 - val_accuracy: 0.1331\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 2.2584 - accuracy: 0.1540 - val_loss: 2.2309 - val_accuracy: 0.1690\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 2.2034 - accuracy: 0.2067 - val_loss: 2.1682 - val_accuracy: 0.2111\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 2.1295 - accuracy: 0.2561 - val_loss: 2.0799 - val_accuracy: 0.2873\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 2.0405 - accuracy: 0.2985 - val_loss: 1.9797 - val_accuracy: 0.3266\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.9597 - accuracy: 0.3270 - val_loss: 1.8895 - val_accuracy: 0.3558\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.8688 - accuracy: 0.3644 - val_loss: 1.8633 - val_accuracy: 0.3465\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 1.7774 - accuracy: 0.3999 - val_loss: 1.8478 - val_accuracy: 0.3607\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.6951 - accuracy: 0.4305 - val_loss: 1.7951 - val_accuracy: 0.3689\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.6024 - accuracy: 0.4714 - val_loss: 1.5221 - val_accuracy: 0.5140\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.5342 - accuracy: 0.4995 - val_loss: 1.4293 - val_accuracy: 0.5466\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 1.4644 - accuracy: 0.5227 - val_loss: 1.5254 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.3972 - accuracy: 0.5481 - val_loss: 1.3180 - val_accuracy: 0.5902\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.3405 - accuracy: 0.5700 - val_loss: 1.3097 - val_accuracy: 0.5826\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 1.3003 - accuracy: 0.5840 - val_loss: 1.2501 - val_accuracy: 0.6077\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 1.2664 - accuracy: 0.5965 - val_loss: 1.2394 - val_accuracy: 0.6111\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.2320 - accuracy: 0.6094 - val_loss: 1.3390 - val_accuracy: 0.5596\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.1956 - accuracy: 0.6242 - val_loss: 1.1555 - val_accuracy: 0.6420\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 1.1629 - accuracy: 0.6367 - val_loss: 1.2378 - val_accuracy: 0.6007\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.1394 - accuracy: 0.6446 - val_loss: 1.1333 - val_accuracy: 0.6445\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 1.1079 - accuracy: 0.6554 - val_loss: 1.0948 - val_accuracy: 0.6598\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.0861 - accuracy: 0.6644 - val_loss: 1.0824 - val_accuracy: 0.6627\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 1.0692 - accuracy: 0.6684 - val_loss: 1.0262 - val_accuracy: 0.6869\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.0550 - accuracy: 0.6743 - val_loss: 1.0400 - val_accuracy: 0.6800\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 1.0325 - accuracy: 0.6825 - val_loss: 1.0098 - val_accuracy: 0.6939\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 1.0101 - accuracy: 0.6887 - val_loss: 1.0282 - val_accuracy: 0.6827\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.9993 - accuracy: 0.6914 - val_loss: 0.9553 - val_accuracy: 0.7112\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.9827 - accuracy: 0.6980 - val_loss: 0.9555 - val_accuracy: 0.7112\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.9678 - accuracy: 0.7030 - val_loss: 0.9652 - val_accuracy: 0.7021\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.9539 - accuracy: 0.7091 - val_loss: 0.9556 - val_accuracy: 0.7080\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.9428 - accuracy: 0.7123 - val_loss: 0.9206 - val_accuracy: 0.7196\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.9298 - accuracy: 0.7153 - val_loss: 0.9255 - val_accuracy: 0.7175\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.9096 - accuracy: 0.7212 - val_loss: 0.9426 - val_accuracy: 0.7096\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.9064 - accuracy: 0.7216 - val_loss: 0.9263 - val_accuracy: 0.7170\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.8984 - accuracy: 0.7248 - val_loss: 0.9005 - val_accuracy: 0.7212\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.8880 - accuracy: 0.7256 - val_loss: 0.8807 - val_accuracy: 0.7344\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.8718 - accuracy: 0.7334 - val_loss: 0.8600 - val_accuracy: 0.7373\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.8642 - accuracy: 0.7357 - val_loss: 0.8430 - val_accuracy: 0.7422\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.8530 - accuracy: 0.7385 - val_loss: 0.9561 - val_accuracy: 0.7057\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.8440 - accuracy: 0.7419 - val_loss: 0.8264 - val_accuracy: 0.7515\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.8380 - accuracy: 0.7434 - val_loss: 0.8734 - val_accuracy: 0.7289\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.8275 - accuracy: 0.7467 - val_loss: 0.8855 - val_accuracy: 0.7296\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.8111 - accuracy: 0.7522 - val_loss: 0.7961 - val_accuracy: 0.7613\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.8053 - accuracy: 0.7544 - val_loss: 0.8303 - val_accuracy: 0.7461\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.7987 - accuracy: 0.7564 - val_loss: 0.7943 - val_accuracy: 0.7593\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.7919 - accuracy: 0.7572 - val_loss: 0.8011 - val_accuracy: 0.7544\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.7838 - accuracy: 0.7591 - val_loss: 0.7820 - val_accuracy: 0.7639\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.7749 - accuracy: 0.7618 - val_loss: 0.7734 - val_accuracy: 0.7656\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.7708 - accuracy: 0.7636 - val_loss: 0.7925 - val_accuracy: 0.7592\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.7608 - accuracy: 0.7676 - val_loss: 0.7756 - val_accuracy: 0.7633\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.7561 - accuracy: 0.7701 - val_loss: 0.7783 - val_accuracy: 0.7627\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.7475 - accuracy: 0.7716 - val_loss: 0.7480 - val_accuracy: 0.7738\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.7396 - accuracy: 0.7738 - val_loss: 0.7546 - val_accuracy: 0.7733\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.7327 - accuracy: 0.7761 - val_loss: 0.7467 - val_accuracy: 0.7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.7225 - accuracy: 0.7794 - val_loss: 0.7934 - val_accuracy: 0.7585\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.7186 - accuracy: 0.7818 - val_loss: 0.7360 - val_accuracy: 0.7781\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.7117 - accuracy: 0.7812 - val_loss: 0.7527 - val_accuracy: 0.7678\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.7062 - accuracy: 0.7825 - val_loss: 0.7131 - val_accuracy: 0.7849\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.6964 - accuracy: 0.7873 - val_loss: 0.7098 - val_accuracy: 0.7852\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.6966 - accuracy: 0.7877 - val_loss: 0.7702 - val_accuracy: 0.7671- accuracy: 0.\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.6851 - accuracy: 0.7901 - val_loss: 0.7055 - val_accuracy: 0.7862\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.6861 - accuracy: 0.7916 - val_loss: 0.6970 - val_accuracy: 0.7884\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.6811 - accuracy: 0.7907 - val_loss: 0.6763 - val_accuracy: 0.7951\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.6719 - accuracy: 0.7954 - val_loss: 0.6754 - val_accuracy: 0.7967\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.6693 - accuracy: 0.7958 - val_loss: 0.6738 - val_accuracy: 0.7977\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.6621 - accuracy: 0.7971 - val_loss: 0.7215 - val_accuracy: 0.7804\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.6563 - accuracy: 0.8001 - val_loss: 0.6656 - val_accuracy: 0.7986\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.6537 - accuracy: 0.7995 - val_loss: 0.6772 - val_accuracy: 0.7950\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.6457 - accuracy: 0.8030 - val_loss: 0.7035 - val_accuracy: 0.7854\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.6415 - accuracy: 0.8043 - val_loss: 0.6580 - val_accuracy: 0.8012\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.6364 - accuracy: 0.8036 - val_loss: 0.6874 - val_accuracy: 0.7923\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.6355 - accuracy: 0.8059 - val_loss: 0.6451 - val_accuracy: 0.8043\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.6249 - accuracy: 0.8082 - val_loss: 0.6400 - val_accuracy: 0.8079\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.6194 - accuracy: 0.8107 - val_loss: 0.6730 - val_accuracy: 0.7957\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.6257 - accuracy: 0.8067 - val_loss: 0.6526 - val_accuracy: 0.8036\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.6152 - accuracy: 0.8109 - val_loss: 0.6326 - val_accuracy: 0.8100\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.6106 - accuracy: 0.8129 - val_loss: 0.6655 - val_accuracy: 0.7995  - ETA: 0s\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.6093 - accuracy: 0.8126 - val_loss: 0.6383 - val_accuracy: 0.8070\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.6043 - accuracy: 0.8157 - val_loss: 0.6792 - val_accuracy: 0.7949\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.6022 - accuracy: 0.8167 - val_loss: 0.6100 - val_accuracy: 0.8175\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.5937 - accuracy: 0.8191 - val_loss: 0.6205 - val_accuracy: 0.8121\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.5936 - accuracy: 0.8180 - val_loss: 0.6006 - val_accuracy: 0.8198\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.5868 - accuracy: 0.8201 - val_loss: 0.6574 - val_accuracy: 0.7982\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.5828 - accuracy: 0.8231 - val_loss: 0.6075 - val_accuracy: 0.8188\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.5785 - accuracy: 0.8250 - val_loss: 0.5996 - val_accuracy: 0.8194\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.5711 - accuracy: 0.8252 - val_loss: 0.6207 - val_accuracy: 0.8112\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.5713 - accuracy: 0.8252 - val_loss: 0.6237 - val_accuracy: 0.8119\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.5686 - accuracy: 0.8269 - val_loss: 0.6595 - val_accuracy: 0.8022\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.5595 - accuracy: 0.8287 - val_loss: 0.5824 - val_accuracy: 0.8247\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.5655 - accuracy: 0.8265 - val_loss: 0.6011 - val_accuracy: 0.8201\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.5559 - accuracy: 0.8288 - val_loss: 0.5962 - val_accuracy: 0.8211\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5486 - accuracy: 0.8323 - val_loss: 0.5896 - val_accuracy: 0.8236\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.5510 - accuracy: 0.8306 - val_loss: 0.5758 - val_accuracy: 0.8269\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.5421 - accuracy: 0.8341 - val_loss: 0.5702 - val_accuracy: 0.8298\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.5440 - accuracy: 0.8336 - val_loss: 0.5990 - val_accuracy: 0.8188\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.5373 - accuracy: 0.8358 - val_loss: 0.5927 - val_accuracy: 0.8213\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.5345 - accuracy: 0.8370 - val_loss: 0.5740 - val_accuracy: 0.8280\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 26s 624us/sample - loss: 0.5323 - accuracy: 0.8378 - val_loss: 0.6583 - val_accuracy: 0.8008\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 25s 587us/sample - loss: 0.5283 - accuracy: 0.8384 - val_loss: 0.5627 - val_accuracy: 0.8308ss: 0.5 - ETA: 1s\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 25s 592us/sample - loss: 0.5221 - accuracy: 0.8407 - val_loss: 0.5432 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model4.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with weight initializers\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.5432 - accuracy: 0.8398\n",
      "Validation accuracy: 83.98\n"
     ]
    }
   ],
   "source": [
    "print('NN with weight initializers'); print('--'*40)\n",
    "results4 = model4.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, Adam optimizers with weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.8857 - accuracy: 0.7328 - val_loss: 0.7426 - val_accuracy: 0.7711\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 0.7428 - accuracy: 0.7677 - val_loss: 0.7723 - val_accuracy: 0.7642\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7257 - accuracy: 0.7737 - val_loss: 0.7729 - val_accuracy: 0.7571\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 0.7211 - accuracy: 0.7775 - val_loss: 0.7252 - val_accuracy: 0.7785\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 0.6986 - accuracy: 0.7843 - val_loss: 0.7285 - val_accuracy: 0.7765\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 0.6800 - accuracy: 0.7891 - val_loss: 0.6845 - val_accuracy: 0.7875\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 4s 89us/sample - loss: 0.6777 - accuracy: 0.7906 - val_loss: 0.6468 - val_accuracy: 0.8027\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.6654 - accuracy: 0.7945 - val_loss: 0.7158 - val_accuracy: 0.7781\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 4s 96us/sample - loss: 0.6321 - accuracy: 0.8038 - val_loss: 0.6112 - val_accuracy: 0.8141\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.6225 - accuracy: 0.8080 - val_loss: 0.6597 - val_accuracy: 0.8002\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 4s 86us/sample - loss: 0.6283 - accuracy: 0.8059 - val_loss: 0.5988 - val_accuracy: 0.8183\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.6155 - accuracy: 0.8091 - val_loss: 0.6284 - val_accuracy: 0.8061\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.5835 - accuracy: 0.8196 - val_loss: 0.6351 - val_accuracy: 0.8068\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 0.5893 - accuracy: 0.8174 - val_loss: 0.6447 - val_accuracy: 0.8026\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.5795 - accuracy: 0.8204 - val_loss: 0.6514 - val_accuracy: 0.7999\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 0.5812 - accuracy: 0.8208 - val_loss: 0.5938 - val_accuracy: 0.8202\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.5696 - accuracy: 0.8229 - val_loss: 0.5545 - val_accuracy: 0.8329\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 4s 94us/sample - loss: 0.5602 - accuracy: 0.8278 - val_loss: 0.5996 - val_accuracy: 0.8213\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 0.5468 - accuracy: 0.8299 - val_loss: 0.5726 - val_accuracy: 0.8256\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.5454 - accuracy: 0.8315 - val_loss: 0.5800 - val_accuracy: 0.8219\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 4s 95us/sample - loss: 0.5300 - accuracy: 0.8366 - val_loss: 0.6216 - val_accuracy: 0.8081\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 0.5289 - accuracy: 0.8355 - val_loss: 0.6311 - val_accuracy: 0.8057\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 0.5121 - accuracy: 0.8394 - val_loss: 0.5516 - val_accuracy: 0.8320\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 0.5156 - accuracy: 0.8391 - val_loss: 0.6238 - val_accuracy: 0.8083\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 0.4960 - accuracy: 0.8457 - val_loss: 0.5432 - val_accuracy: 0.8352\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 3s 70us/sample - loss: 0.5050 - accuracy: 0.8413 - val_loss: 0.5266 - val_accuracy: 0.8421\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 0.4983 - accuracy: 0.8439 - val_loss: 0.5316 - val_accuracy: 0.8397\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 0.4910 - accuracy: 0.8474 - val_loss: 0.5691 - val_accuracy: 0.8239\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 0.4880 - accuracy: 0.8475 - val_loss: 0.5215 - val_accuracy: 0.8423\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 4s 94us/sample - loss: 0.4771 - accuracy: 0.8498 - val_loss: 0.5156 - val_accuracy: 0.8428\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 0.4711 - accuracy: 0.8515 - val_loss: 0.5947 - val_accuracy: 0.8173\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.4630 - accuracy: 0.8573 - val_loss: 0.4888 - val_accuracy: 0.8530\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 0.4447 - accuracy: 0.8607 - val_loss: 0.4871 - val_accuracy: 0.8518\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 0.4432 - accuracy: 0.8615 - val_loss: 0.5394 - val_accuracy: 0.8334\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.4400 - accuracy: 0.8613 - val_loss: 0.4980 - val_accuracy: 0.8506\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.4417 - accuracy: 0.8625 - val_loss: 0.5164 - val_accuracy: 0.8440\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 3s 83us/sample - loss: 0.4344 - accuracy: 0.8636 - val_loss: 0.4864 - val_accuracy: 0.8517\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 4s 87us/sample - loss: 0.4318 - accuracy: 0.8637 - val_loss: 0.5091 - val_accuracy: 0.8460\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.4323 - accuracy: 0.8628 - val_loss: 0.4468 - val_accuracy: 0.8651\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 4s 90us/sample - loss: 0.4205 - accuracy: 0.8663 - val_loss: 0.4544 - val_accuracy: 0.8616\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 0.4291 - accuracy: 0.8651 - val_loss: 0.4586 - val_accuracy: 0.8621\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.4043 - accuracy: 0.8741 - val_loss: 0.4679 - val_accuracy: 0.8576\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.4055 - accuracy: 0.8724 - val_loss: 0.4691 - val_accuracy: 0.8570\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 4s 87us/sample - loss: 0.3876 - accuracy: 0.8782 - val_loss: 0.4722 - val_accuracy: 0.8550\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 4s 93us/sample - loss: 0.4048 - accuracy: 0.8712 - val_loss: 0.4518 - val_accuracy: 0.8638\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.4028 - accuracy: 0.8709 - val_loss: 0.4947 - val_accuracy: 0.8462\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.3816 - accuracy: 0.8792 - val_loss: 0.4515 - val_accuracy: 0.8662\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.3914 - accuracy: 0.8751 - val_loss: 0.4803 - val_accuracy: 0.8559\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 4s 90us/sample - loss: 0.3853 - accuracy: 0.8780 - val_loss: 0.4463 - val_accuracy: 0.8686\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.3836 - accuracy: 0.8772 - val_loss: 0.4365 - val_accuracy: 0.8695\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 82us/sample - loss: 0.3778 - accuracy: 0.8803 - val_loss: 0.4362 - val_accuracy: 0.8692\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 4s 97us/sample - loss: 0.3694 - accuracy: 0.8822 - val_loss: 0.4149 - val_accuracy: 0.8754\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 4s 96us/sample - loss: 0.3575 - accuracy: 0.8857 - val_loss: 0.4599 - val_accuracy: 0.8633s - loss: 0\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 4s 87us/sample - loss: 0.3774 - accuracy: 0.8792 - val_loss: 0.4443 - val_accuracy: 0.8670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 4s 95us/sample - loss: 0.3612 - accuracy: 0.8836 - val_loss: 0.3950 - val_accuracy: 0.8828\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 4s 95us/sample - loss: 0.3571 - accuracy: 0.8850 - val_loss: 0.4724 - val_accuracy: 0.8577\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 4s 84us/sample - loss: 0.3674 - accuracy: 0.8825 - val_loss: 0.4461 - val_accuracy: 0.8631\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 0.3483 - accuracy: 0.8888 - val_loss: 0.4261 - val_accuracy: 0.8711\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.3482 - accuracy: 0.8878 - val_loss: 0.4613 - val_accuracy: 0.8603\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.3520 - accuracy: 0.8885 - val_loss: 0.4376 - val_accuracy: 0.8682\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 3s 83us/sample - loss: 0.3482 - accuracy: 0.8887 - val_loss: 0.4192 - val_accuracy: 0.8777\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 4s 95us/sample - loss: 0.3395 - accuracy: 0.8908 - val_loss: 0.4049 - val_accuracy: 0.8824\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 4s 89us/sample - loss: 0.3403 - accuracy: 0.8900 - val_loss: 0.4111 - val_accuracy: 0.8793\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 4s 90us/sample - loss: 0.3509 - accuracy: 0.8878 - val_loss: 0.4046 - val_accuracy: 0.8809\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.3390 - accuracy: 0.8897 - val_loss: 0.4087 - val_accuracy: 0.8799\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.3263 - accuracy: 0.8945 - val_loss: 0.4479 - val_accuracy: 0.8654s - loss: 0.3234 - accuracy: \n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.3198 - accuracy: 0.8972 - val_loss: 0.4022 - val_accuracy: 0.8832\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.3226 - accuracy: 0.8962 - val_loss: 0.4214 - val_accuracy: 0.8751\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 4s 96us/sample - loss: 0.3029 - accuracy: 0.9009 - val_loss: 0.3985 - val_accuracy: 0.8842\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 4s 86us/sample - loss: 0.3162 - accuracy: 0.8980 - val_loss: 0.4200 - val_accuracy: 0.8795loss: 0.3078  - ETA: 0s - l\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.3136 - accuracy: 0.8980 - val_loss: 0.4359 - val_accuracy: 0.8684\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 3s 81us/sample - loss: 0.3155 - accuracy: 0.8981 - val_loss: 0.3923 - val_accuracy: 0.8881\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 4s 97us/sample - loss: 0.3166 - accuracy: 0.8970 - val_loss: 0.3892 - val_accuracy: 0.8883\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 4s 97us/sample - loss: 0.3096 - accuracy: 0.8996 - val_loss: 0.4110 - val_accuracy: 0.8819\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 79us/sample - loss: 0.3096 - accuracy: 0.8996 - val_loss: 0.4440 - val_accuracy: 0.8691\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.3023 - accuracy: 0.9016 - val_loss: 0.4086 - val_accuracy: 0.8809\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 78us/sample - loss: 0.3034 - accuracy: 0.9020 - val_loss: 0.4088 - val_accuracy: 0.8788\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.2973 - accuracy: 0.9021 - val_loss: 0.4218 - val_accuracy: 0.8774\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 4s 85us/sample - loss: 0.2837 - accuracy: 0.9074 - val_loss: 0.3969 - val_accuracy: 0.8861\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.2988 - accuracy: 0.9021 - val_loss: 0.3901 - val_accuracy: 0.8878\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 4s 92us/sample - loss: 0.3070 - accuracy: 0.8995 - val_loss: 0.3795 - val_accuracy: 0.8915\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.2872 - accuracy: 0.9067 - val_loss: 0.3853 - val_accuracy: 0.8921\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 4s 88us/sample - loss: 0.2740 - accuracy: 0.9124 - val_loss: 0.3798 - val_accuracy: 0.8911\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 5s 118us/sample - loss: 0.2780 - accuracy: 0.9098 - val_loss: 0.4012 - val_accuracy: 0.8862\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2786 - accuracy: 0.9093 - val_loss: 0.3943 - val_accuracy: 0.8872\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.2862 - accuracy: 0.9062 - val_loss: 0.3813 - val_accuracy: 0.8920\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 4s 91us/sample - loss: 0.2728 - accuracy: 0.9116 - val_loss: 0.3910 - val_accuracy: 0.8903\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.2666 - accuracy: 0.9129 - val_loss: 0.3711 - val_accuracy: 0.8949\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 5s 117us/sample - loss: 0.2721 - accuracy: 0.9108 - val_loss: 0.4407 - val_accuracy: 0.8749\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.2760 - accuracy: 0.9095 - val_loss: 0.3601 - val_accuracy: 0.9020\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.2602 - accuracy: 0.9151 - val_loss: 0.3636 - val_accuracy: 0.8980\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 4s 96us/sample - loss: 0.2669 - accuracy: 0.9130 - val_loss: 0.3866 - val_accuracy: 0.8942\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.2648 - accuracy: 0.9129 - val_loss: 0.4150 - val_accuracy: 0.8831\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 5s 112us/sample - loss: 0.2511 - accuracy: 0.9173 - val_loss: 0.3999 - val_accuracy: 0.8905\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.2707 - accuracy: 0.9110 - val_loss: 0.3821 - val_accuracy: 0.8945\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 4s 89us/sample - loss: 0.2545 - accuracy: 0.9159 - val_loss: 0.3870 - val_accuracy: 0.8917\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.2500 - accuracy: 0.9199 - val_loss: 0.4065 - val_accuracy: 0.8844\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.2557 - accuracy: 0.9141 - val_loss: 0.3634 - val_accuracy: 0.9004\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.2506 - accuracy: 0.9167 - val_loss: 0.4031 - val_accuracy: 0.8886\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 80us/sample - loss: 0.2508 - accuracy: 0.9168 - val_loss: 0.4118 - val_accuracy: 0.8835\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model4.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with weight initializers\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.4118 - accuracy: 0.8835\n",
      "Validation accuracy: 88.35\n"
     ]
    }
   ],
   "source": [
    "print('NN with weight initializers'); print('--'*40)\n",
    "results4 = model4.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation - Weight initializers\n",
    "Adding weight initialiers didn't result in improvement of score.\n",
    "relu activations, changing number of activators, Adam optimizers gives the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, SGD optimizers with weight initializers and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with batch normalization\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with batch normalization'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model5 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
    "model5.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model5.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - adding second hidden layer\n",
    "model5.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 3 - adding third hidden layer\n",
    "model5.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model5.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model5.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 307,882\n",
      "Trainable params: 306,922\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 7s 174us/sample - loss: 2.2898 - accuracy: 0.1962 - val_loss: 2.2100 - val_accuracy: 0.1857\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 5s 115us/sample - loss: 1.8474 - accuracy: 0.3758 - val_loss: 1.8042 - val_accuracy: 0.3970\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 4s 93us/sample - loss: 1.5986 - accuracy: 0.4849 - val_loss: 1.5742 - val_accuracy: 0.4904\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 1.4198 - accuracy: 0.5589 - val_loss: 1.4119 - val_accuracy: 0.5533\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 1.2817 - accuracy: 0.6091 - val_loss: 1.3251 - val_accuracy: 0.5797\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 1.1687 - accuracy: 0.6451 - val_loss: 1.1797 - val_accuracy: 0.6309\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 1.0805 - accuracy: 0.6698 - val_loss: 1.1369 - val_accuracy: 0.6385\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 6s 133us/sample - loss: 1.0066 - accuracy: 0.6911 - val_loss: 1.0860 - val_accuracy: 0.6568\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.9473 - accuracy: 0.7123 - val_loss: 1.0620 - val_accuracy: 0.6662\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.8956 - accuracy: 0.7257 - val_loss: 1.1123 - val_accuracy: 0.6410\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.8522 - accuracy: 0.7364 - val_loss: 0.9859 - val_accuracy: 0.6810\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.8153 - accuracy: 0.7475 - val_loss: 0.9859 - val_accuracy: 0.6831\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 6s 131us/sample - loss: 0.7794 - accuracy: 0.7588 - val_loss: 0.9284 - val_accuracy: 0.6980\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.7515 - accuracy: 0.7671 - val_loss: 0.8452 - val_accuracy: 0.7311\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.7233 - accuracy: 0.7751 - val_loss: 0.8441 - val_accuracy: 0.7318\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.6996 - accuracy: 0.7819 - val_loss: 0.9108 - val_accuracy: 0.7026\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.6725 - accuracy: 0.7909 - val_loss: 0.9537 - val_accuracy: 0.6929\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.6567 - accuracy: 0.7958 - val_loss: 0.8070 - val_accuracy: 0.7404\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.6312 - accuracy: 0.8036 - val_loss: 0.9502 - val_accuracy: 0.6978\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.6207 - accuracy: 0.8058 - val_loss: 0.8345 - val_accuracy: 0.7320\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 6s 145us/sample - loss: 0.6045 - accuracy: 0.8108 - val_loss: 0.7443 - val_accuracy: 0.7641\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.5881 - accuracy: 0.8168 - val_loss: 0.9107 - val_accuracy: 0.7111\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.5718 - accuracy: 0.8202 - val_loss: 0.8022 - val_accuracy: 0.7453\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.5582 - accuracy: 0.8257 - val_loss: 0.7095 - val_accuracy: 0.7740\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.5477 - accuracy: 0.8299 - val_loss: 0.9872 - val_accuracy: 0.6958\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.5369 - accuracy: 0.8310 - val_loss: 0.7792 - val_accuracy: 0.7512\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.5227 - accuracy: 0.8371 - val_loss: 0.6645 - val_accuracy: 0.7896\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.5163 - accuracy: 0.8388 - val_loss: 0.7094 - val_accuracy: 0.7761\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.5003 - accuracy: 0.8433 - val_loss: 0.7242 - val_accuracy: 0.7714\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.4929 - accuracy: 0.8455 - val_loss: 0.6639 - val_accuracy: 0.7891\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.4840 - accuracy: 0.8471 - val_loss: 0.7255 - val_accuracy: 0.7705\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.4712 - accuracy: 0.8536 - val_loss: 0.7461 - val_accuracy: 0.7622\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.4656 - accuracy: 0.8539 - val_loss: 0.8053 - val_accuracy: 0.7473\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 6s 144us/sample - loss: 0.4567 - accuracy: 0.8557 - val_loss: 0.6544 - val_accuracy: 0.7911\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.4418 - accuracy: 0.8634 - val_loss: 0.5927 - val_accuracy: 0.8135\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.4375 - accuracy: 0.8623 - val_loss: 0.8985 - val_accuracy: 0.7247\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.4296 - accuracy: 0.8650 - val_loss: 0.6984 - val_accuracy: 0.7774\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 6s 141us/sample - loss: 0.4252 - accuracy: 0.8657 - val_loss: 0.6175 - val_accuracy: 0.8058\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.4199 - accuracy: 0.8684 - val_loss: 0.7909 - val_accuracy: 0.7510\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 6s 142us/sample - loss: 0.4099 - accuracy: 0.8718 - val_loss: 1.0189 - val_accuracy: 0.7028\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.3995 - accuracy: 0.8753 - val_loss: 0.6846 - val_accuracy: 0.7893\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.3953 - accuracy: 0.8761 - val_loss: 0.6393 - val_accuracy: 0.7983\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 6s 154us/sample - loss: 0.3913 - accuracy: 0.8771 - val_loss: 0.8223 - val_accuracy: 0.7515\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 6s 133us/sample - loss: 0.3800 - accuracy: 0.8799 - val_loss: 0.6803 - val_accuracy: 0.7912\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 6s 146us/sample - loss: 0.3752 - accuracy: 0.8820 - val_loss: 0.5655 - val_accuracy: 0.8267\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.3684 - accuracy: 0.8848 - val_loss: 0.5875 - val_accuracy: 0.8143\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 6s 134us/sample - loss: 0.3613 - accuracy: 0.8877 - val_loss: 0.6732 - val_accuracy: 0.7932\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 6s 155us/sample - loss: 0.3542 - accuracy: 0.8885 - val_loss: 0.5963 - val_accuracy: 0.8153\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 7s 156us/sample - loss: 0.3551 - accuracy: 0.8887 - val_loss: 0.6689 - val_accuracy: 0.7969\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.3465 - accuracy: 0.8916 - val_loss: 1.0954 - val_accuracy: 0.6960\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 6s 133us/sample - loss: 0.3378 - accuracy: 0.8946 - val_loss: 0.5980 - val_accuracy: 0.8168\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 6s 143us/sample - loss: 0.3365 - accuracy: 0.8938 - val_loss: 0.6290 - val_accuracy: 0.8045\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 6s 146us/sample - loss: 0.3307 - accuracy: 0.8968 - val_loss: 0.6244 - val_accuracy: 0.8102\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.3242 - accuracy: 0.8985 - val_loss: 0.7820 - val_accuracy: 0.7737\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.3247 - accuracy: 0.8983 - val_loss: 0.5951 - val_accuracy: 0.8126\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 6s 153us/sample - loss: 0.3160 - accuracy: 0.9016 - val_loss: 0.7519 - val_accuracy: 0.7695\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.3105 - accuracy: 0.9040 - val_loss: 0.8796 - val_accuracy: 0.7598\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.3122 - accuracy: 0.9027 - val_loss: 0.6695 - val_accuracy: 0.7950\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.3020 - accuracy: 0.9061 - val_loss: 0.6439 - val_accuracy: 0.8036\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.2975 - accuracy: 0.9078 - val_loss: 0.8024 - val_accuracy: 0.7694\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.2914 - accuracy: 0.9099 - val_loss: 0.6122 - val_accuracy: 0.8123\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.2923 - accuracy: 0.9090 - val_loss: 0.5631 - val_accuracy: 0.8280\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.2882 - accuracy: 0.9105 - val_loss: 0.6091 - val_accuracy: 0.8148\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2779 - accuracy: 0.9137 - val_loss: 0.6068 - val_accuracy: 0.8194\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 6s 134us/sample - loss: 0.2731 - accuracy: 0.9157 - val_loss: 0.6061 - val_accuracy: 0.8140\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.2733 - accuracy: 0.9153 - val_loss: 0.5950 - val_accuracy: 0.8177\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.2656 - accuracy: 0.9176 - val_loss: 0.6550 - val_accuracy: 0.8001\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.2611 - accuracy: 0.9195 - val_loss: 0.9665 - val_accuracy: 0.7394\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2617 - accuracy: 0.9184 - val_loss: 0.7882 - val_accuracy: 0.7742\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 5s 131us/sample - loss: 0.2604 - accuracy: 0.9198 - val_loss: 0.5405 - val_accuracy: 0.8369\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 6s 145us/sample - loss: 0.2489 - accuracy: 0.9235 - val_loss: 0.5436 - val_accuracy: 0.8387\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 6s 131us/sample - loss: 0.2503 - accuracy: 0.9228 - val_loss: 1.0765 - val_accuracy: 0.7203\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2484 - accuracy: 0.9236 - val_loss: 0.5948 - val_accuracy: 0.8190\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 6s 149us/sample - loss: 0.2417 - accuracy: 0.9262 - val_loss: 1.5662 - val_accuracy: 0.6477\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.2411 - accuracy: 0.9249 - val_loss: 0.5953 - val_accuracy: 0.8242\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.2357 - accuracy: 0.9282 - val_loss: 0.5328 - val_accuracy: 0.8409\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.2372 - accuracy: 0.9269 - val_loss: 0.7430 - val_accuracy: 0.7829\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.2338 - accuracy: 0.9272 - val_loss: 0.5915 - val_accuracy: 0.8210\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.2260 - accuracy: 0.9308 - val_loss: 0.6333 - val_accuracy: 0.8221\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.2306 - accuracy: 0.9291 - val_loss: 0.8697 - val_accuracy: 0.7608\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.2178 - accuracy: 0.9330 - val_loss: 0.5207 - val_accuracy: 0.8482\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.2194 - accuracy: 0.9328 - val_loss: 0.7759 - val_accuracy: 0.7810\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 6s 144us/sample - loss: 0.2159 - accuracy: 0.9318 - val_loss: 0.7217 - val_accuracy: 0.8027\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.2154 - accuracy: 0.9344 - val_loss: 0.5084 - val_accuracy: 0.8501\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.2085 - accuracy: 0.9364 - val_loss: 0.7293 - val_accuracy: 0.7944\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2057 - accuracy: 0.9372 - val_loss: 0.6263 - val_accuracy: 0.8165\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.2061 - accuracy: 0.9363 - val_loss: 1.3620 - val_accuracy: 0.6957\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.2048 - accuracy: 0.9369 - val_loss: 0.5510 - val_accuracy: 0.8371\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2013 - accuracy: 0.9381 - val_loss: 0.7875 - val_accuracy: 0.7843\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 6s 150us/sample - loss: 0.1985 - accuracy: 0.9393 - val_loss: 0.9587 - val_accuracy: 0.7508\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 6s 139us/sample - loss: 0.1939 - accuracy: 0.9397 - val_loss: 0.8883 - val_accuracy: 0.7670\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 6s 143us/sample - loss: 0.1937 - accuracy: 0.9404 - val_loss: 0.6691 - val_accuracy: 0.8132\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 6s 137us/sample - loss: 0.1923 - accuracy: 0.9409 - val_loss: 0.6238 - val_accuracy: 0.8235\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.1885 - accuracy: 0.9420 - val_loss: 0.4861 - val_accuracy: 0.8591\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 5s 131us/sample - loss: 0.1833 - accuracy: 0.9447 - val_loss: 0.8294 - val_accuracy: 0.7844\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 6s 153us/sample - loss: 0.1807 - accuracy: 0.9451 - val_loss: 0.6564 - val_accuracy: 0.8186\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.1826 - accuracy: 0.9439 - val_loss: 0.4448 - val_accuracy: 0.8718\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.1794 - accuracy: 0.9448 - val_loss: 0.5963 - val_accuracy: 0.8314\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1759 - accuracy: 0.9463 - val_loss: 0.5525 - val_accuracy: 0.8426\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 6s 152us/sample - loss: 0.1738 - accuracy: 0.9471 - val_loss: 0.6026 - val_accuracy: 0.8274\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model5.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with batch normalization\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.6026 - accuracy: 0.8274\n",
      "Validation accuracy: 82.74\n"
     ]
    }
   ],
   "source": [
    "print('NN with batch normalization'); print('--'*40)\n",
    "results5 = model5.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model, relu activations, Adam optimizers with weight initializers and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.7695 - accuracy: 0.7679 - val_loss: 3.2194 - val_accuracy: 0.3952\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 6s 131us/sample - loss: 0.5856 - accuracy: 0.8123 - val_loss: 2.3795 - val_accuracy: 0.4437\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.5175 - accuracy: 0.8335 - val_loss: 1.6642 - val_accuracy: 0.5087\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 6s 149us/sample - loss: 0.4801 - accuracy: 0.8437 - val_loss: 1.2810 - val_accuracy: 0.6074\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 6s 142us/sample - loss: 0.4581 - accuracy: 0.8485 - val_loss: 1.7298 - val_accuracy: 0.5363\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.4432 - accuracy: 0.8567 - val_loss: 1.3371 - val_accuracy: 0.6169\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.4234 - accuracy: 0.8630 - val_loss: 1.3090 - val_accuracy: 0.6154\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 6s 150us/sample - loss: 0.4001 - accuracy: 0.8700 - val_loss: 1.1338 - val_accuracy: 0.6516\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.3797 - accuracy: 0.8771 - val_loss: 1.0445 - val_accuracy: 0.6721\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.3700 - accuracy: 0.8794 - val_loss: 1.6445 - val_accuracy: 0.5842\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.3537 - accuracy: 0.8839 - val_loss: 1.1592 - val_accuracy: 0.6497\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.3499 - accuracy: 0.8857 - val_loss: 1.0108 - val_accuracy: 0.6885\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.3444 - accuracy: 0.8874 - val_loss: 1.5105 - val_accuracy: 0.5986\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 6s 140us/sample - loss: 0.3249 - accuracy: 0.8948 - val_loss: 0.9327 - val_accuracy: 0.7120\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 6s 149us/sample - loss: 0.3169 - accuracy: 0.8972 - val_loss: 1.2770 - val_accuracy: 0.6525\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.3111 - accuracy: 0.8976 - val_loss: 1.3478 - val_accuracy: 0.6395\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 6s 134us/sample - loss: 0.3017 - accuracy: 0.9003 - val_loss: 1.3639 - val_accuracy: 0.6154\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.2867 - accuracy: 0.9063 - val_loss: 1.1198 - val_accuracy: 0.6723\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2914 - accuracy: 0.9043 - val_loss: 1.3934 - val_accuracy: 0.6661\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 6s 141us/sample - loss: 0.2744 - accuracy: 0.9094 - val_loss: 1.5511 - val_accuracy: 0.6085\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.2743 - accuracy: 0.9088 - val_loss: 1.1590 - val_accuracy: 0.6744\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 6s 145us/sample - loss: 0.2585 - accuracy: 0.9162 - val_loss: 1.0784 - val_accuracy: 0.7097\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.2557 - accuracy: 0.9158 - val_loss: 1.0436 - val_accuracy: 0.7083\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2544 - accuracy: 0.9167 - val_loss: 0.9490 - val_accuracy: 0.7261\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 6s 139us/sample - loss: 0.2402 - accuracy: 0.9213 - val_loss: 1.1045 - val_accuracy: 0.6953\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 6s 154us/sample - loss: 0.2406 - accuracy: 0.9203 - val_loss: 1.0507 - val_accuracy: 0.7177\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.2417 - accuracy: 0.9210 - val_loss: 1.2165 - val_accuracy: 0.6689\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 6s 143us/sample - loss: 0.2283 - accuracy: 0.9237 - val_loss: 0.9729 - val_accuracy: 0.7220\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 7s 164us/sample - loss: 0.2165 - accuracy: 0.9284 - val_loss: 1.5446 - val_accuracy: 0.6282\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 6s 142us/sample - loss: 0.2159 - accuracy: 0.9285 - val_loss: 1.3091 - val_accuracy: 0.6527\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.2074 - accuracy: 0.9306 - val_loss: 1.1707 - val_accuracy: 0.7030\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.2100 - accuracy: 0.9308 - val_loss: 1.7258 - val_accuracy: 0.6235\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 7s 162us/sample - loss: 0.2125 - accuracy: 0.9290 - val_loss: 0.8692 - val_accuracy: 0.7603\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.1985 - accuracy: 0.9335 - val_loss: 0.9928 - val_accuracy: 0.7474\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 7s 166us/sample - loss: 0.1855 - accuracy: 0.9382 - val_loss: 1.0946 - val_accuracy: 0.7338\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 6s 137us/sample - loss: 0.1873 - accuracy: 0.9390 - val_loss: 1.1542 - val_accuracy: 0.7204\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.1739 - accuracy: 0.9418 - val_loss: 1.4578 - val_accuracy: 0.6844\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.1764 - accuracy: 0.9407 - val_loss: 0.7166 - val_accuracy: 0.7986\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.1735 - accuracy: 0.9418 - val_loss: 0.9575 - val_accuracy: 0.7706\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 6s 150us/sample - loss: 0.1767 - accuracy: 0.9398 - val_loss: 0.7395 - val_accuracy: 0.7953\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 6s 146us/sample - loss: 0.1679 - accuracy: 0.9443 - val_loss: 0.8329 - val_accuracy: 0.7713s: 0.1669 - ac\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 6s 149us/sample - loss: 0.1760 - accuracy: 0.9405 - val_loss: 1.4236 - val_accuracy: 0.6646\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 6s 134us/sample - loss: 0.1700 - accuracy: 0.9423 - val_loss: 0.9025 - val_accuracy: 0.7542\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.1536 - accuracy: 0.9491 - val_loss: 0.9269 - val_accuracy: 0.7778 - loss: 0.1535 - \n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.1486 - accuracy: 0.9501 - val_loss: 0.8901 - val_accuracy: 0.7781\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 6s 134us/sample - loss: 0.1535 - accuracy: 0.9485 - val_loss: 0.8201 - val_accuracy: 0.7878\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 6s 153us/sample - loss: 0.1568 - accuracy: 0.9483 - val_loss: 0.8618 - val_accuracy: 0.7751\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.1394 - accuracy: 0.9542 - val_loss: 0.7226 - val_accuracy: 0.8080\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.1382 - accuracy: 0.9543 - val_loss: 0.7903 - val_accuracy: 0.8025\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 6s 150us/sample - loss: 0.1463 - accuracy: 0.9510 - val_loss: 0.9591 - val_accuracy: 0.7720\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.1521 - accuracy: 0.9489 - val_loss: 0.6802 - val_accuracy: 0.8290\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.1325 - accuracy: 0.9560 - val_loss: 1.6345 - val_accuracy: 0.6558\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.1340 - accuracy: 0.9548 - val_loss: 1.0262 - val_accuracy: 0.7607\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.1352 - accuracy: 0.9550 - val_loss: 1.3593 - val_accuracy: 0.7182\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 7s 159us/sample - loss: 0.1365 - accuracy: 0.9540 - val_loss: 0.7279 - val_accuracy: 0.8144\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.1275 - accuracy: 0.9577 - val_loss: 1.1347 - val_accuracy: 0.7406\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 6s 150us/sample - loss: 0.1359 - accuracy: 0.9545 - val_loss: 1.0339 - val_accuracy: 0.7691\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.1230 - accuracy: 0.9588 - val_loss: 0.9713 - val_accuracy: 0.7832\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 5s 131us/sample - loss: 0.1178 - accuracy: 0.9602 - val_loss: 0.9741 - val_accuracy: 0.7703\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.1235 - accuracy: 0.9587 - val_loss: 0.8362 - val_accuracy: 0.8054\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.1153 - accuracy: 0.9619 - val_loss: 0.9753 - val_accuracy: 0.7707\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 6s 139us/sample - loss: 0.1194 - accuracy: 0.9600 - val_loss: 1.6322 - val_accuracy: 0.6945\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 6s 144us/sample - loss: 0.1158 - accuracy: 0.9611 - val_loss: 0.7877 - val_accuracy: 0.8067\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.1154 - accuracy: 0.9618 - val_loss: 1.1287 - val_accuracy: 0.7452\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 5s 131us/sample - loss: 0.1209 - accuracy: 0.9591 - val_loss: 0.7064 - val_accuracy: 0.8238\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 6s 137us/sample - loss: 0.1075 - accuracy: 0.9640 - val_loss: 0.7538 - val_accuracy: 0.8218\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.1055 - accuracy: 0.9646 - val_loss: 0.9398 - val_accuracy: 0.7663\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 6s 146us/sample - loss: 0.1103 - accuracy: 0.9623 - val_loss: 0.9424 - val_accuracy: 0.7885\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.1114 - accuracy: 0.9632 - val_loss: 1.2242 - val_accuracy: 0.7525 0.1113 - \n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 6s 151us/sample - loss: 0.1011 - accuracy: 0.9653 - val_loss: 1.0393 - val_accuracy: 0.7722\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 6s 149us/sample - loss: 0.0989 - accuracy: 0.9662 - val_loss: 0.7888 - val_accuracy: 0.8095 - ETA: 0s - loss: 0.0987 - accuracy: 0.96\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.0996 - accuracy: 0.9669 - val_loss: 0.8118 - val_accuracy: 0.8115\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 7s 161us/sample - loss: 0.1080 - accuracy: 0.9628 - val_loss: 0.9033 - val_accuracy: 0.7938\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1092 - accuracy: 0.9629 - val_loss: 1.2070 - val_accuracy: 0.7558\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.1085 - accuracy: 0.9630 - val_loss: 0.7735 - val_accuracy: 0.8156\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.0986 - accuracy: 0.9670 - val_loss: 0.7592 - val_accuracy: 0.8180\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 6s 137us/sample - loss: 0.1018 - accuracy: 0.9659 - val_loss: 0.8240 - val_accuracy: 0.8083\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 7s 156us/sample - loss: 0.0881 - accuracy: 0.9701 - val_loss: 1.0541 - val_accuracy: 0.7733\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 6s 152us/sample - loss: 0.0994 - accuracy: 0.9669 - val_loss: 0.9510 - val_accuracy: 0.7899\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.0879 - accuracy: 0.9704 - val_loss: 0.8631 - val_accuracy: 0.8044\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.0978 - accuracy: 0.9661 - val_loss: 1.0377 - val_accuracy: 0.7780\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0934 - accuracy: 0.9678 - val_loss: 0.8917 - val_accuracy: 0.7988\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 6s 138us/sample - loss: 0.0930 - accuracy: 0.9687 - val_loss: 0.7661 - val_accuracy: 0.8241\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.0910 - accuracy: 0.9693 - val_loss: 0.9712 - val_accuracy: 0.7923\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 6s 147us/sample - loss: 0.0834 - accuracy: 0.9718 - val_loss: 1.1433 - val_accuracy: 0.7684\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.0926 - accuracy: 0.9680 - val_loss: 0.9209 - val_accuracy: 0.8038\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 7s 160us/sample - loss: 0.0857 - accuracy: 0.9714 - val_loss: 0.6090 - val_accuracy: 0.8488\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 6s 152us/sample - loss: 0.0847 - accuracy: 0.9712 - val_loss: 0.9804 - val_accuracy: 0.7990\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 6s 133us/sample - loss: 0.0804 - accuracy: 0.9725 - val_loss: 0.7050 - val_accuracy: 0.8466\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 6s 135us/sample - loss: 0.0818 - accuracy: 0.9723 - val_loss: 1.1409 - val_accuracy: 0.7779\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 7s 171us/sample - loss: 0.0866 - accuracy: 0.9704 - val_loss: 1.0925 - val_accuracy: 0.7758\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 6s 136us/sample - loss: 0.0822 - accuracy: 0.9721 - val_loss: 0.8779 - val_accuracy: 0.8061\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 7s 158us/sample - loss: 0.0833 - accuracy: 0.9727 - val_loss: 1.2338 - val_accuracy: 0.7675835 - accuracy\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0780 - accuracy: 0.9735 - val_loss: 1.2498 - val_accuracy: 0.7594\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 6s 154us/sample - loss: 0.0960 - accuracy: 0.9673 - val_loss: 1.2111 - val_accuracy: 0.7686\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 6s 139us/sample - loss: 0.0804 - accuracy: 0.9727 - val_loss: 0.5860 - val_accuracy: 0.86980.97\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 7s 157us/sample - loss: 0.0761 - accuracy: 0.9745 - val_loss: 0.9106 - val_accuracy: 0.8124\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 6s 133us/sample - loss: 0.0792 - accuracy: 0.9734 - val_loss: 0.7110 - val_accuracy: 0.8448\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 6s 148us/sample - loss: 0.0744 - accuracy: 0.9744 - val_loss: 1.4355 - val_accuracy: 0.7608\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 6s 141us/sample - loss: 0.0736 - accuracy: 0.9747 - val_loss: 1.1268 - val_accuracy: 0.7831\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model5.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with batch normalization\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 1.1268 - accuracy: 0.7831\n",
      "Validation accuracy: 78.31\n"
     ]
    }
   ],
   "source": [
    "print('NN with batch normalization'); print('--'*40)\n",
    "results5 = model5.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation - Batch Normalization\n",
    "Batch normalization didn't result in improvement of score.\n",
    "Relu activations, changing number of activators, Adam optimizers achieved the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-eca90d4f3763>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-65-eca90d4f3763>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    NN model, relu activations, SGD optimizers with weight initializers, batch normalization and dropout\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NN model, relu activations, SGD optimizers with weight initializers, batch normalization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with dropout - sgd optimizer\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model6 = Sequential()\n",
    "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
    "model6.add(Dense(512, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization()) \n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model6.add(Dense(256, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 2 - adding second hidden layer\n",
    "model6.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 3 - adding third hidden layer\n",
    "model6.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 4 - adding fourth hidden layer\n",
    "model6.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model6.add(Dense(10, kernel_initializer = 'he_normal',bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model6.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 2.5744 - accuracy: 0.1135 - val_loss: 2.2965 - val_accuracy: 0.1258\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 2.3791 - accuracy: 0.1400 - val_loss: 2.1914 - val_accuracy: 0.2013\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 2.2794 - accuracy: 0.1686 - val_loss: 2.0897 - val_accuracy: 0.2682\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 2.2026 - accuracy: 0.1918 - val_loss: 1.9912 - val_accuracy: 0.3123\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 2.1201 - accuracy: 0.2275 - val_loss: 1.8875 - val_accuracy: 0.3640\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 10s 242us/sample - loss: 2.0413 - accuracy: 0.2599 - val_loss: 1.7965 - val_accuracy: 0.4083\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 1.9631 - accuracy: 0.2919 - val_loss: 1.7125 - val_accuracy: 0.4388\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 1.8984 - accuracy: 0.3178 - val_loss: 1.6154 - val_accuracy: 0.4956\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 11s 260us/sample - loss: 1.8270 - accuracy: 0.3458 - val_loss: 1.5522 - val_accuracy: 0.5156\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 1.7629 - accuracy: 0.3741 - val_loss: 1.4728 - val_accuracy: 0.5423\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 1.7075 - accuracy: 0.3957 - val_loss: 1.4122 - val_accuracy: 0.5673\n",
      "Epoch 12/100\n",
      "39200/42000 [===========================>..] - ETA: 0s - loss: 1.6536 - accuracy: 0.4164"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model6.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
    "results6 = model6.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model6.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NN model with dropout - adam optimizer'); print('--'*40)\n",
    "results6 = model6.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation - Batch Normalization and Dropout\n",
    "No improvement of score.\n",
    "NN model, relu activations, SGD optimizers with weight initializers and batch normalization is still the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on test dataset using Model 3 - relu activations, Adam optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model3 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu\n",
    "model3.add(Dense(256, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model3.add(Dense(128))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - Adding second hidden layer\n",
    "model3.add(Dense(64))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model3.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NN with batch normalization'); print('--'*40)\n",
    "results3 = model3.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing the model on test dataset')\n",
    "predictions = model3.predict_classes(X_test)\n",
    "score = model3.evaluate(X_test, y_test)\n",
    "print('Test loss :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report'); print('--'*40)\n",
    "print(classification_report(y_test_o, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visualizing the confusion matrix')\n",
    "plt.figure(figsize = (15, 7.2))\n",
    "sns.heatmap(confusion_matrix(y_test_o, predictions), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model3 Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model3 Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.predict_classes(X_test)[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the image\n",
    "plt.imshow(X_test[20].reshape(32, 32), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.predict_classes(X_test)[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[10].reshape(32, 32), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.predict_classes(X_test)[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "* Baby sitting process achieved the best accuracy of 21% using hyper parameter optimization.\n",
    "* NN through API method achieved best accuracy of 90% on validation set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
